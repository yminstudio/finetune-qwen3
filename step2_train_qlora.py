#!/usr/bin/env python3
"""
ğŸ”§ 2ë‹¨ê³„: QLora íŒŒì¸íŠœë‹ ì‹¤í–‰ (2-3ì‹œê°„)

ëª©ì :
- í•œêµ­ì–´ ê¸°ìˆ  QA ë°ì´í„°ë¡œ ëª¨ë¸ íŠ¹í™”
- LoRA ì–´ëŒ‘í„° ìƒì„± ë° í•™ìŠµ
- í•™ìŠµ ê³¼ì • ëª¨ë‹ˆí„°ë§ ë° ìµœì í™”
"""

import os
import sys
import time
import json
import torch
import gc
from pathlib import Path
from datasets import Dataset, load_dataset
from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM, 
    BitsAndBytesConfig,
    TrainingArguments
)
from peft import LoraConfig, get_peft_model, TaskType
from trl import SFTTrainer
import psutil
import GPUtil
from config import PROJECT_ROOT

# ì„¤ì • íŒŒì¼ import
from config import *

LOG_DIR = os.path.join(os.path.dirname(__file__), 'logs')
LOG_FILE = os.path.join(LOG_DIR, 'step2_train_qlora.log')
os.makedirs(LOG_DIR, exist_ok=True)
def log_print(*args, **kwargs):
    print(*args, **kwargs)
    with open(LOG_FILE, 'a', encoding='utf-8') as f:
        print(*args, **kwargs, file=f)

def monitor_resources():
    """ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§"""
    # GPU ë©”ëª¨ë¦¬
    if torch.cuda.is_available():
        gpu_memory = torch.cuda.memory_allocated() / (1024**3)
        gpu_cached = torch.cuda.memory_reserved() / (1024**3)
        
        # GPUtilë¡œ GPU ì‚¬ìš©ë¥  í™•ì¸
        gpus = GPUtil.getGPUs()
        gpu_usage = (gpus[0].memoryUtil * 100) if gpus else 0
        
        log_print(f"   GPU: {gpu_memory:.1f}GB í• ë‹¹, {gpu_cached:.1f}GB ìºì‹œ, ì‚¬ìš©ë¥ : {gpu_usage:.1f}%")
    
    # ì‹œìŠ¤í…œ RAM
    memory = psutil.virtual_memory()
    log_print(f"   RAM: {memory.used/(1024**3):.1f}GB / {memory.total/(1024**3):.1f}GB ({memory.percent:.1f}%)")

def load_and_prepare_data():
    """í•™ìŠµ ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬"""
    log_print("ğŸ“Š [1/6] í•™ìŠµ ë°ì´í„° ì¤€ë¹„ ì‹œì‘...")
    start_time = time.time()
    
    # ì ˆëŒ€ ê²½ë¡œë¡œ ë³€ê²½
    data_path = PROJECT_ROOT / "data" / "korean_tech_qa.json"
    if not data_path.exists():
        log_print("âŒ í•™ìŠµ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        log_print(f"   íŒŒì¼ ê²½ë¡œ: {data_path}")
        log_print("   ë‹¤ìŒ ëª…ë ¹ìœ¼ë¡œ ë°ì´í„°ë¥¼ ìƒì„±í•˜ì„¸ìš”: python create_sample_data.py")
        return None
    
    # JSON ë°ì´í„° ë¡œë“œ
    log_print("   ğŸ“ JSON íŒŒì¼ ë¡œë“œ ì¤‘...")
    with open(data_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    log_print(f"   âœ… ë¡œë“œëœ ìƒ˜í”Œ ìˆ˜: {len(data)}ê°œ")
    
    # ChatML í˜•ì‹ì„ Qwen3 chat templateë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜
    def format_chat_template(example):
        messages = example['messages']
        return {"messages": messages}
    
    # Dataset ê°ì²´ ìƒì„±
    log_print("   ğŸ”„ ë°ì´í„°ì…‹ í¬ë§·íŒ… ì¤‘...")
    dataset = Dataset.from_list(data)
    dataset = dataset.map(format_chat_template)
    
    elapsed_time = time.time() - start_time
    log_print(f"   âœ… ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ! (ì†Œìš”ì‹œê°„: {elapsed_time:.1f}ì´ˆ)")
    log_print(f"   ğŸ“‹ ë°ì´í„°ì…‹ í¬ê¸°: {len(dataset)}")
    log_print(f"   ğŸ“‹ ì²« ë²ˆì§¸ ìƒ˜í”Œ ë©”ì‹œì§€ ìˆ˜: {len(dataset[0]['messages'])}")
    
    return dataset

def setup_model_and_tokenizer():
    """ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ì„¤ì •"""
    log_print(f"\nğŸ¤– [2/6] ëª¨ë¸ ë¡œë“œ ì‹œì‘: {MODEL_NAME}")
    start_time = time.time()
    
    # BitsAndBytesConfig ì„¤ì •
    log_print("   âš™ï¸ 4-bit ì–‘ìí™” ì„¤ì • ì¤‘...")
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16,
        bnb_4bit_use_double_quant=False,
    )
    
    # í† í¬ë‚˜ì´ì € ë¡œë“œ
    log_print("   ğŸ“ í† í¬ë‚˜ì´ì € ë¡œë“œ ì¤‘...")
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "right"
    log_print("   âœ… í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ")
    
    # ëª¨ë¸ ë¡œë“œ
    log_print("   ğŸ”„ ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ ì¤‘ (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)...")
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_NAME,
        quantization_config=bnb_config,
        torch_dtype=torch.bfloat16,
        device_map="auto",
        trust_remote_code=True
    )
    
    model_load_time = time.time() - start_time
    log_print(f"   âœ… ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ! (ì†Œìš”ì‹œê°„: {model_load_time:.1f}ì´ˆ)")
    log_print(f"   ğŸ“Š ëª¨ë¸ íƒ€ì…: {type(model)}")
    log_print(f"   ğŸ“Š ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´: {MAX_SEQ_LENGTH}")
    
    # í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶œë ¥
    monitor_resources()
    
    # LoRA ì„¤ì •
    log_print("\nğŸ”§ [3/6] LoRA ì–´ëŒ‘í„° ì„¤ì • ì‹œì‘...")
    lora_start_time = time.time()
    
    lora_config = LoraConfig(
        r=QLORA_CONFIG["r"],
        lora_alpha=QLORA_CONFIG["lora_alpha"],
        target_modules=QLORA_CONFIG["target_modules"],
        lora_dropout=QLORA_CONFIG["lora_dropout"],
        bias=QLORA_CONFIG["bias"],
        task_type=TaskType.CAUSAL_LM,
    )
    
    log_print(f"   âš™ï¸ LoRA ì„¤ì •: rank={QLORA_CONFIG['r']}, alpha={QLORA_CONFIG['lora_alpha']}")
    log_print(f"   ğŸ¯ Target modules: {QLORA_CONFIG['target_modules']}")
    
    # PEFT ëª¨ë¸ë¡œ ë³€í™˜
    log_print("   ğŸ”„ PEFT ëª¨ë¸ ë³€í™˜ ì¤‘...")
    model = get_peft_model(model, lora_config)
    
    # í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° ìˆ˜ ì¶œë ¥
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    total_params = sum(p.numel() for p in model.parameters())
    percentage = 100 * trainable_params / total_params
    
    lora_time = time.time() - lora_start_time
    total_time = time.time() - start_time
    
    log_print(f"   âœ… LoRA ì–´ëŒ‘í„° ì„¤ì • ì™„ë£Œ! (ì†Œìš”ì‹œê°„: {lora_time:.1f}ì´ˆ)")
    log_print(f"   ğŸ“Š í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°: {trainable_params:,} ({percentage:.3f}%)")
    log_print(f"   ğŸ“Š ì „ì²´ íŒŒë¼ë¯¸í„°: {total_params:,}")
    log_print(f"   â±ï¸ ì´ ëª¨ë¸ ì¤€ë¹„ ì‹œê°„: {total_time:.1f}ì´ˆ")
    
    # LoRA ì„¤ì • í›„ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
    monitor_resources()
    
    return model, tokenizer

def setup_trainer(model, tokenizer, dataset):
    """SFTTrainer ì„¤ì •"""
    log_print("\nğŸ‹ï¸ [4/6] íŠ¸ë ˆì´ë„ˆ ì„¤ì • ì‹œì‘...")
    start_time = time.time()
    
    # í•™ìŠµ ì¸ì ì„¤ì •
    log_print("   âš™ï¸ í•™ìŠµ ë§¤ê°œë³€ìˆ˜ ì„¤ì • ì¤‘...")
    training_args = TrainingArguments(**TRAINING_CONFIG)
    
     # chat template ì ìš© í•¨ìˆ˜ (ë‹¨ì¼ ì˜ˆì œ ì²˜ë¦¬)
    def formatting_prompts_func(example):
        # exampleì€ ë‹¨ì¼ ë”•ì…”ë„ˆë¦¬ {"messages": [...]}
        messages = example["messages"]
        
        # ì§ì ‘ í…ìŠ¤íŠ¸ í¬ë§·íŒ… (chat template ëŒ€ì‹ )
        formatted_text = ""
        for message in messages:
            role = message["role"]
            content = message["content"]
            formatted_text += f"<|im_start|>{role}\n{content}<|im_end|>\n"
        
        return {"text": formatted_text}
    
    log_print("   ğŸ”„ SFTTrainer ì´ˆê¸°í™” ì¤‘...")
    # SFTTrainer ìƒì„±
    trainer = SFTTrainer(
        model=model,
        train_dataset=dataset,
        args=training_args,
        formatting_func=formatting_prompts_func,
    )
    
    elapsed_time = time.time() - start_time
    log_print(f"   âœ… íŠ¸ë ˆì´ë„ˆ ì„¤ì • ì™„ë£Œ! (ì†Œìš”ì‹œê°„: {elapsed_time:.1f}ì´ˆ)")
    log_print(f"   ğŸ“Š ë°°ì¹˜ í¬ê¸°: {training_args.per_device_train_batch_size}")
    log_print(f"   ğŸ“Š gradient accumulation: {training_args.gradient_accumulation_steps}")
    log_print(f"   ğŸ“Š ì‹¤íš¨ ë°°ì¹˜ í¬ê¸°: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}")
    log_print(f"   ğŸ“Š ìµœëŒ€ ìŠ¤í…: {training_args.max_steps}")
    log_print(f"   ğŸ“Š í•™ìŠµë¥ : {training_args.learning_rate}")
    
    return trainer

def train_model(trainer):
    """ëª¨ë¸ í•™ìŠµ ì‹¤í–‰"""
    log_print("\nğŸš€ [5/6] í•™ìŠµ ì‹œì‘!")
    log_print("=" * 60)
    
    start_time = time.time()
    
    # ì´ˆê¸° ë¦¬ì†ŒìŠ¤ ìƒíƒœ
    log_print("ğŸ“Š í•™ìŠµ ì‹œì‘ ì „ ë¦¬ì†ŒìŠ¤ ìƒíƒœ:")
    monitor_resources()
    log_print()
    
    log_print("ğŸ”¥ QLora íŒŒì¸íŠœë‹ ì‹¤í–‰ ì¤‘...")
    log_print("   - ì˜ˆìƒ ì†Œìš”ì‹œê°„: 20-30ë¶„")
    log_print("   - 100 ìŠ¤í… í•™ìŠµ ì§„í–‰")
    log_print("   - 50ìŠ¤í…, 100ìŠ¤í…ì—ì„œ ì²´í¬í¬ì¸íŠ¸ ì €ì¥")
    log_print()
    
    # í•™ìŠµ ì‹¤í–‰
    try:
        trainer.train()
    except Exception as e:
        log_print(f"âŒ í•™ìŠµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    elapsed_time = time.time() - start_time
    hours = elapsed_time // 3600
    minutes = (elapsed_time % 3600) // 60
    
    log_print("=" * 60)
    log_print(f"âœ… í•™ìŠµ ì™„ë£Œ! ì†Œìš” ì‹œê°„: {int(hours)}ì‹œê°„ {int(minutes)}ë¶„")
    
    # ìµœì¢… ë¦¬ì†ŒìŠ¤ ìƒíƒœ
    log_print("\nğŸ“Š í•™ìŠµ ì™„ë£Œ í›„ ë¦¬ì†ŒìŠ¤ ìƒíƒœ:")
    monitor_resources()
    
    return True

def save_model_and_adapters(model, tokenizer):
    """ëª¨ë¸ê³¼ ì–´ëŒ‘í„° ì €ì¥"""
    log_print("\nğŸ’¾ [6/6] ëª¨ë¸ ì €ì¥ ì‹œì‘...")
    start_time = time.time()
    
    # LoRA ì–´ëŒ‘í„°ë§Œ ì €ì¥
    lora_output_dir = OUTPUT_DIR / "qwen3_lora_adapters"
    lora_output_dir.mkdir(exist_ok=True)
    
    log_print("   ğŸ“ LoRA ì–´ëŒ‘í„° ì €ì¥ ì¤‘...")
    model.save_pretrained(str(lora_output_dir))
    
    log_print("   ğŸ“ í† í¬ë‚˜ì´ì € ì €ì¥ ì¤‘...")
    tokenizer.save_pretrained(str(lora_output_dir))
    
    save_time = time.time() - start_time
    log_print(f"   âœ… LoRA ì–´ëŒ‘í„° ì €ì¥ ì™„ë£Œ: {lora_output_dir} (ì†Œìš”ì‹œê°„: {save_time:.1f}ì´ˆ)")
    
    # ì €ì¥ëœ íŒŒì¼ í¬ê¸° í™•ì¸
    adapter_file = lora_output_dir / "adapter_model.safetensors"
    if adapter_file.exists():
        size_gb = adapter_file.stat().st_size / (1024**3)
        log_print(f"   ğŸ“Š ì–´ëŒ‘í„° íŒŒì¼ í¬ê¸°: {size_gb:.2f}GB")
    
    # í•™ìŠµ ê²°ê³¼ ìš”ì•½ ì €ì¥
    log_print("   ğŸ“‹ í•™ìŠµ ìš”ì•½ ì €ì¥ ì¤‘...")
    summary = {
        "model_name": MODEL_NAME,
        "lora_config": QLORA_CONFIG,
        "training_config": TRAINING_CONFIG,
        "output_dir": str(lora_output_dir),
        "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
    }
    
    summary_file = lora_output_dir / "training_summary.json"
    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump(summary, f, ensure_ascii=False, indent=2)
    
    total_save_time = time.time() - start_time
    log_print(f"   âœ… í•™ìŠµ ìš”ì•½ ì €ì¥ ì™„ë£Œ: {summary_file} (ì´ ì €ì¥ì‹œê°„: {total_save_time:.1f}ì´ˆ)")
    
    return lora_output_dir

def cleanup_memory():
    """ë©”ëª¨ë¦¬ ì •ë¦¬"""
    log_print("ğŸ§¹ ë©”ëª¨ë¦¬ ì •ë¦¬ ì¤‘...")
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    
    log_print("   ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    overall_start_time = time.time()
    
    log_print("ğŸ”§ 2ë‹¨ê³„: QLora íŒŒì¸íŠœë‹ ì‹œì‘")
    log_print("=" * 60)
    log_print("ğŸ¯ ëª©í‘œ: Qwen3 ëª¨ë¸ì„ í•œêµ­ì–´ ê¸°ìˆ  QAë¡œ íŒŒì¸íŠœë‹")
    log_print("â±ï¸ ì˜ˆìƒ ì´ ì†Œìš”ì‹œê°„: 30-40ë¶„")
    log_print("=" * 60)
    
    try:
        # 1. ë°ì´í„° ì¤€ë¹„
        dataset = load_and_prepare_data()
        if dataset is None:
            log_print("âŒ 1ë‹¨ê³„ ì‹¤íŒ¨: ë°ì´í„° ì¤€ë¹„")
            return False
        
        # 2. ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ì„¤ì •
        model, tokenizer = setup_model_and_tokenizer()
        
        # 3. íŠ¸ë ˆì´ë„ˆ ì„¤ì •
        trainer = setup_trainer(model, tokenizer, dataset)
        
        # 4. í•™ìŠµ ì‹¤í–‰
        if not train_model(trainer):
            log_print("âŒ 5ë‹¨ê³„ ì‹¤íŒ¨: í•™ìŠµ ì‹¤í–‰")
            return False
        
        # 5. ëª¨ë¸ ì €ì¥
        output_dir = save_model_and_adapters(model, tokenizer)
        
        # 6. ë©”ëª¨ë¦¬ ì •ë¦¬
        cleanup_memory()
        
        # ì „ì²´ ì™„ë£Œ ì‹œê°„ ê³„ì‚°
        total_elapsed = time.time() - overall_start_time
        hours = total_elapsed // 3600
        minutes = (total_elapsed % 3600) // 60
        
        log_print("\n" + "=" * 60)
        log_print("ğŸ‰ 2ë‹¨ê³„ QLoRA íŒŒì¸íŠœë‹ ì™„ì „ ì™„ë£Œ!")
        log_print("=" * 60)
        log_print(f"â±ï¸ ì´ ì†Œìš”ì‹œê°„: {int(hours)}ì‹œê°„ {int(minutes)}ë¶„")
        log_print(f"ğŸ“ LoRA ì–´ëŒ‘í„° ìœ„ì¹˜: {output_dir}")
        log_print(f"ğŸ“Š ì–´ëŒ‘í„° í¬ê¸°: ì•½ 3GB")
        log_print("=" * 60)
        log_print("ğŸ“‹ ë‹¤ìŒ ë‹¨ê³„ ì•ˆë‚´:")
        log_print("   âœ… step3 ì‹¤í–‰: python step3_validate_results.py")
        log_print("   ğŸ” í•™ìŠµ íš¨ê³¼ ê²€ì¦ ë° ì¶”ë¡  ì†ë„ ì¸¡ì •")
        log_print("=" * 60)
        
        return True
        
    except Exception as e:
        elapsed = time.time() - overall_start_time
        log_print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ (ì‹¤í–‰ì‹œê°„: {elapsed:.1f}ì´ˆ)")
        log_print(f"ğŸ” ì˜¤ë¥˜ ë‚´ìš©: {e}")
        log_print("=" * 60)
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1) 