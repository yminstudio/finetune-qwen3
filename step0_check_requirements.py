#!/usr/bin/env python3
"""
ğŸ” Step 0: ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­ ì²´í¬

íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì „ í•„ìˆ˜ ìš”êµ¬ì‚¬í•­ì„ ìë™ìœ¼ë¡œ í™•ì¸í•©ë‹ˆë‹¤:
- GPU ë©”ëª¨ë¦¬ 45GB+ í™•ì¸
- ì‹œìŠ¤í…œ RAM 64GB+ í™•ì¸  
- ë””ìŠ¤í¬ ì—¬ìœ ê³µê°„ 150GB+ í™•ì¸
- CUDA ì„¤ì¹˜ í™•ì¸
- Python í™˜ê²½ í™•ì¸
"""

import os
import sys
import subprocess
import platform
import shutil
from pathlib import Path

def print_banner():
    """ì²´í¬ ì‹œì‘ ë°°ë„ˆ"""
    banner = """
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                  â•‘
â•‘  ğŸ” ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­ ì²´í¬ (Step 0)                                   â•‘
â•‘                                                                  â•‘
â•‘  QLora íŒŒì¸íŠœë‹ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ì„ ìœ„í•œ í•„ìˆ˜ ì¡°ê±´ë“¤ì„ í™•ì¸í•©ë‹ˆë‹¤      â•‘
â•‘                                                                  â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""
    print(banner)

def check_python_version():
    """Python ë²„ì „ í™•ì¸"""
    print("ğŸ Python ë²„ì „ í™•ì¸ ì¤‘...")
    
    version = sys.version_info
    python_version = f"{version.major}.{version.minor}.{version.micro}"
    
    print(f"   í˜„ì¬ Python ë²„ì „: {python_version}")
    
    if version >= (3, 8):
        print("   âœ… Python 3.8+ ìš”êµ¬ì‚¬í•­ ë§Œì¡±")
        return True
    else:
        print("   âŒ Python 3.8 ì´ìƒì´ í•„ìš”í•©ë‹ˆë‹¤")
        print("   ğŸ’¡ í•´ê²°ë°©ë²•: Python 3.8+ ì„¤ì¹˜")
        return False

def check_system_info():
    """ì‹œìŠ¤í…œ ê¸°ë³¸ ì •ë³´ ì¶œë ¥"""
    print("\nğŸ’» ì‹œìŠ¤í…œ ì •ë³´:")
    print(f"   OS: {platform.system()} {platform.release()}")
    print(f"   ì•„í‚¤í…ì²˜: {platform.machine()}")
    print(f"   í”„ë¡œì„¸ì„œ: {platform.processor()}")

def check_ram():
    """ì‹œìŠ¤í…œ RAM í™•ì¸"""
    print("\nğŸ’¾ ì‹œìŠ¤í…œ RAM í™•ì¸ ì¤‘...")
    
    try:
        import psutil
        
        memory = psutil.virtual_memory()
        total_gb = memory.total / (1024**3)
        available_gb = memory.available / (1024**3)
        used_gb = memory.used / (1024**3)
        
        print(f"   ì´ RAM: {total_gb:.1f}GB")
        print(f"   ì‚¬ìš© ì¤‘: {used_gb:.1f}GB")
        print(f"   ì‚¬ìš© ê°€ëŠ¥: {available_gb:.1f}GB")
        
        if total_gb >= 64:
            print("   âœ… RAM 64GB+ ìš”êµ¬ì‚¬í•­ ë§Œì¡±")
            return True
        elif total_gb >= 48:
            print("   âš ï¸ RAM 48GB-64GB: ìµœì†Œ ìš”êµ¬ì‚¬í•­ ë§Œì¡± (ê¶Œì¥: 64GB+)")
            return True
        else:
            print("   âŒ RAM ë¶€ì¡±: ìµœì†Œ 48GB í•„ìš”, ê¶Œì¥ 64GB+")
            print("   ğŸ’¡ í•´ê²°ë°©ë²•: ì‹œìŠ¤í…œ RAM ì—…ê·¸ë ˆì´ë“œ")
            return False
            
    except ImportError:
        print("   âŒ psutil íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        print("   ğŸ’¡ í•´ê²°ë°©ë²•: pip install psutil")
        return False

def check_disk_space():
    """ë””ìŠ¤í¬ ì—¬ìœ ê³µê°„ í™•ì¸"""
    print("\nğŸ’½ ë””ìŠ¤í¬ ì—¬ìœ ê³µê°„ í™•ì¸ ì¤‘...")
    
    try:
        import psutil
        
        # í˜„ì¬ ë””ë ‰í† ë¦¬ì˜ ë””ìŠ¤í¬ ì‚¬ìš©ëŸ‰ í™•ì¸
        current_path = Path.cwd()
        disk_usage = psutil.disk_usage(current_path)
        
        total_gb = disk_usage.total / (1024**3)
        used_gb = disk_usage.used / (1024**3)
        free_gb = disk_usage.free / (1024**3)
        
        print(f"   ë””ìŠ¤í¬ ê²½ë¡œ: {current_path}")
        print(f"   ì´ ìš©ëŸ‰: {total_gb:.1f}GB")
        print(f"   ì‚¬ìš© ì¤‘: {used_gb:.1f}GB")
        print(f"   ì—¬ìœ  ê³µê°„: {free_gb:.1f}GB")
        
        if free_gb >= 150:
            print("   âœ… ë””ìŠ¤í¬ 150GB+ ìš”êµ¬ì‚¬í•­ ë§Œì¡±")
            return True
        elif free_gb >= 100:
            print("   âš ï¸ ë””ìŠ¤í¬ 100GB-150GB: ìµœì†Œ ìš”êµ¬ì‚¬í•­ ë§Œì¡±")
            return True
        else:
            print("   âŒ ë””ìŠ¤í¬ ê³µê°„ ë¶€ì¡±: ìµœì†Œ 150GB í•„ìš”")
            print("   ğŸ’¡ í•´ê²°ë°©ë²•: ë””ìŠ¤í¬ ì •ë¦¬ ë˜ëŠ” ì¶”ê°€ ì €ì¥ê³µê°„ í™•ë³´")
            return False
            
    except ImportError:
        print("   âŒ psutil íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        return False

def check_cuda():
    """CUDA ì„¤ì¹˜ í™•ì¸"""
    print("\nâš¡ CUDA ì„¤ì¹˜ í™•ì¸ ì¤‘...")
    
    # nvidia-smi ëª…ë ¹ì–´ í™•ì¸
    try:
        result = subprocess.run(['nvidia-smi'], capture_output=True, text=True, check=True)
        print("   âœ… nvidia-smi ëª…ë ¹ì–´ ì‚¬ìš© ê°€ëŠ¥")
        
        # CUDA ë²„ì „ ì •ë³´ ì¶”ì¶œ
        output_lines = result.stdout.split('\n')
        for line in output_lines:
            if 'CUDA Version:' in line:
                cuda_version = line.split('CUDA Version:')[1].strip().split()[0]
                print(f"   CUDA ë“œë¼ì´ë²„ ë²„ì „: {cuda_version}")
                break
                
    except (subprocess.CalledProcessError, FileNotFoundError):
        print("   âŒ nvidia-smi ëª…ë ¹ì–´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        print("   ğŸ’¡ í•´ê²°ë°©ë²•: NVIDIA GPU ë“œë¼ì´ë²„ ì„¤ì¹˜")
        return False
    
    # nvcc ëª…ë ¹ì–´ í™•ì¸
    try:
        result = subprocess.run(['nvcc', '--version'], capture_output=True, text=True, check=True)
        nvcc_output = result.stdout
        for line in nvcc_output.split('\n'):
            if 'release' in line:
                cuda_toolkit_version = line.split('release')[1].split(',')[0].strip()
                print(f"   CUDA Toolkit ë²„ì „: {cuda_toolkit_version}")
                break
        print("   âœ… CUDA Toolkit ì„¤ì¹˜ í™•ì¸")
        
    except (subprocess.CalledProcessError, FileNotFoundError):
        print("   âš ï¸ nvcc ëª…ë ¹ì–´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤ (CUDA Toolkit ë¯¸ì„¤ì¹˜)")
        print("   ğŸ’¡ PyTorchë¡œ CUDA ì§€ì› í™•ì¸ ì¤‘...")
    
    return True

def check_gpu():
    """GPU ë©”ëª¨ë¦¬ í™•ì¸"""
    print("\nğŸ® GPU í™•ì¸ ì¤‘...")
    
    try:
        import torch
        
        if not torch.cuda.is_available():
            print("   âŒ CUDA GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            print("   ğŸ’¡ í•´ê²°ë°©ë²•: CUDA í˜¸í™˜ GPU ë° PyTorch CUDA ë²„ì „ ì„¤ì¹˜")
            return False
        
        gpu_count = torch.cuda.device_count()
        print(f"   ì‚¬ìš© ê°€ëŠ¥í•œ GPU ìˆ˜: {gpu_count}ê°œ")
        
        total_memory_gb = 0
        gpu_pass = False
        
        for i in range(gpu_count):
            gpu_name = torch.cuda.get_device_name(i)
            gpu_memory = torch.cuda.get_device_properties(i).total_memory
            gpu_memory_gb = gpu_memory / (1024**3)
            
            print(f"   GPU {i}: {gpu_name}")
            print(f"   VRAM: {gpu_memory_gb:.1f}GB")
            
            total_memory_gb += gpu_memory_gb
            
            if gpu_memory_gb >= 45:
                gpu_pass = True
        
        print(f"   ì´ VRAM: {total_memory_gb:.1f}GB")
        
        if gpu_pass:
            print("   âœ… GPU ë©”ëª¨ë¦¬ 45GB+ ìš”êµ¬ì‚¬í•­ ë§Œì¡±")
            return True
        elif total_memory_gb >= 40:
            print("   âš ï¸ GPU ë©”ëª¨ë¦¬ 40GB-45GB: ê±°ì˜ ë§Œì¡± (ì¼ë¶€ ì œí•œ ê°€ëŠ¥)")
            return True
        else:
            print("   âŒ GPU ë©”ëª¨ë¦¬ ë¶€ì¡±: 45GB+ VRAM í•„ìš”")
            print("   ğŸ’¡ ê¶Œì¥ GPU: RTX A6000 (48GB), H100 (80GB), A100 (40GB/80GB)")
            return False
            
    except ImportError:
        print("   âŒ PyTorchê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        print("   ğŸ’¡ í•´ê²°ë°©ë²•: pip install torch")
        return False
    except Exception as e:
        print(f"   âŒ GPU í™•ì¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return False

def check_gpu_detailed():
    """ìƒì„¸ GPU ì •ë³´ í™•ì¸"""
    print("\nğŸ” ìƒì„¸ GPU ì •ë³´:")
    
    try:
        import GPUtil
        
        gpus = GPUtil.getGPUs()
        if not gpus:
            print("   GPUtilë¡œ GPU ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return
        
        for gpu in gpus:
            print(f"   GPU {gpu.id}: {gpu.name}")
            print(f"   ë©”ëª¨ë¦¬: {gpu.memoryTotal:.0f}MB (ì‚¬ìš©ì¤‘: {gpu.memoryUsed:.0f}MB)")
            print(f"   ì˜¨ë„: {gpu.temperature}Â°C")
            print(f"   ì‚¬ìš©ë¥ : {gpu.load*100:.1f}%")
            
    except ImportError:
        print("   GPUtil íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
    except Exception as e:
        print(f"   ìƒì„¸ GPU ì •ë³´ í™•ì¸ ì‹¤íŒ¨: {e}")

def check_essential_packages():
    """í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì—¬ë¶€ í™•ì¸"""
    print("\nğŸ“¦ í•„ìˆ˜ íŒ¨í‚¤ì§€ í™•ì¸ ì¤‘...")
    
    essential_packages = [
        'torch',
        'transformers', 
        'accelerate',
        'bitsandbytes',
        'peft',
        'datasets',
        'psutil',
        'GPUtil'
    ]
    
    missing_packages = []
    
    for package in essential_packages:
        try:
            __import__(package)
            print(f"   âœ… {package}: ì„¤ì¹˜ë¨")
        except ImportError:
            print(f"   âŒ {package}: ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ")
            missing_packages.append(package)
    
    if missing_packages:
        print(f"\n   ğŸ’¡ ëˆ„ë½ëœ íŒ¨í‚¤ì§€: {', '.join(missing_packages)}")
        print("   í•´ê²°ë°©ë²•: pip install -r requirements.txt")
        return False
    else:
        print("   âœ… ëª¨ë“  í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜ í™•ì¸")
        return True

def check_network():
    """ë„¤íŠ¸ì›Œí¬ ì—°ê²° í™•ì¸"""
    print("\nğŸŒ ë„¤íŠ¸ì›Œí¬ ì—°ê²° í™•ì¸ ì¤‘...")
    
    try:
        import requests
        
        # Hugging Face Hub ì—°ê²° í™•ì¸
        response = requests.get("https://huggingface.co", timeout=10)
        if response.status_code == 200:
            print("   âœ… Hugging Face Hub ì—°ê²° ê°€ëŠ¥")
        else:
            print("   âš ï¸ Hugging Face Hub ì—°ê²° í™•ì¸ í•„ìš”")
            
        # GitHub ì—°ê²° í™•ì¸ (llama.cpp ë‹¤ìš´ë¡œë“œìš©)
        response = requests.get("https://github.com", timeout=10)
        if response.status_code == 200:
            print("   âœ… GitHub ì—°ê²° ê°€ëŠ¥")
        else:
            print("   âš ï¸ GitHub ì—°ê²° í™•ì¸ í•„ìš”")
            
        return True
        
    except ImportError:
        print("   âŒ requests íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        return False
    except Exception as e:
        print(f"   âš ï¸ ë„¤íŠ¸ì›Œí¬ ì—°ê²° í™•ì¸ ì‹¤íŒ¨: {e}")
        return True  # ë„¤íŠ¸ì›Œí¬ëŠ” í•„ìˆ˜ê°€ ì•„ë‹ˆë¯€ë¡œ True ë°˜í™˜

def estimate_time_and_space():
    """ì˜ˆìƒ ì†Œìš”ì‹œê°„ ë° ê³µê°„ ê³„ì‚°"""
    print("\nâ±ï¸ ì˜ˆìƒ ì†Œìš”ì‹œê°„ ë° ê³µê°„:")
    
    print("   ğŸ“Š ë‹¨ê³„ë³„ ì˜ˆìƒ ì‹œê°„:")
    print("   1ë‹¨ê³„ (í™˜ê²½ ì„¤ì •): 30ë¶„")
    print("   2ë‹¨ê³„ (QLora í•™ìŠµ): 2-3ì‹œê°„")
    print("   3ë‹¨ê³„ (ê²°ê³¼ ê²€ì¦): 30ë¶„")
    print("   4ë‹¨ê³„ (ëª¨ë¸ ë³‘í•©): 1ì‹œê°„")
    print("   5ë‹¨ê³„ (GGUF ë³€í™˜): 1-2ì‹œê°„")
    print("   6ë‹¨ê³„ (ì„œë¹„ìŠ¤ ë°°í¬): 1ì‹œê°„")
    print("   ğŸ’¡ ì´ ì†Œìš”ì‹œê°„: 6-8ì‹œê°„")
    
    print("\n   ğŸ’¾ ì˜ˆìƒ ì €ì¥ê³µê°„:")
    print("   ì›ë³¸ ëª¨ë¸ ìºì‹œ: ~30GB")
    print("   LoRA ì–´ëŒ‘í„°: ~3GB")
    print("   ë³‘í•©ëœ ëª¨ë¸: ~60GB")
    print("   GGUF ëª¨ë¸: ~20GB")
    print("   ì„ì‹œ íŒŒì¼ë“¤: ~10GB")
    print("   ğŸ’¡ ì´ í•„ìš”ê³µê°„: ~120-150GB")

def provide_solutions():
    """ë¬¸ì œ í•´ê²° ë°©ë²• ì œì‹œ"""
    print("\nğŸ› ï¸ ì¼ë°˜ì ì¸ ë¬¸ì œ í•´ê²° ë°©ë²•:")
    
    print("\n   GPU ë©”ëª¨ë¦¬ ë¶€ì¡±:")
    print("   - gradient_accumulation_steps ëŠ˜ë¦¬ê¸°")
    print("   - per_device_train_batch_size ì¤„ì´ê¸°")
    print("   - max_seq_length ì¤„ì´ê¸°")
    
    print("\n   ì‹œìŠ¤í…œ RAM ë¶€ì¡±:")
    print("   - ë¸Œë¼ìš°ì € ë° ë¶ˆí•„ìš”í•œ í”„ë¡œê·¸ë¨ ì¢…ë£Œ")
    print("   - swap ë©”ëª¨ë¦¬ í™œì„±í™”")
    print("   - max_steps ì¤„ì—¬ì„œ ë‹¨ê³„ë³„ ì‹¤í–‰")
    
    print("\n   ë””ìŠ¤í¬ ê³µê°„ ë¶€ì¡±:")
    print("   - ë¶ˆí•„ìš”í•œ íŒŒì¼ ì‚­ì œ")
    print("   - ì™¸ì¥ ë“œë¼ì´ë¸Œ í™œìš©")
    print("   - Docker ìºì‹œ ì •ë¦¬")
    
    print("\n   CUDA ë¬¸ì œ:")
    print("   - NVIDIA ë“œë¼ì´ë²„ ì¬ì„¤ì¹˜")
    print("   - PyTorch CUDA ë²„ì „ í™•ì¸")
    print("   - CUDA Toolkit ì„¤ì¹˜")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print_banner()
    
    # ì‹œìŠ¤í…œ ì •ë³´ ì¶œë ¥
    check_system_info()
    
    # ê° ìš”êµ¬ì‚¬í•­ ì²´í¬
    checks = []
    
    checks.append(("Python ë²„ì „", check_python_version()))
    checks.append(("ì‹œìŠ¤í…œ RAM", check_ram()))
    checks.append(("ë””ìŠ¤í¬ ê³µê°„", check_disk_space()))
    checks.append(("CUDA ì„¤ì¹˜", check_cuda()))
    checks.append(("GPU ë©”ëª¨ë¦¬", check_gpu()))
    checks.append(("í•„ìˆ˜ íŒ¨í‚¤ì§€", check_essential_packages()))
    checks.append(("ë„¤íŠ¸ì›Œí¬ ì—°ê²°", check_network()))
    
    # ìƒì„¸ GPU ì •ë³´
    check_gpu_detailed()
    
    # ì˜ˆìƒ ì‹œê°„ ë° ê³µê°„
    estimate_time_and_space()
    
    # ê²°ê³¼ ìš”ì•½
    print("\n" + "="*70)
    print("ğŸ“‹ ìš”êµ¬ì‚¬í•­ ì²´í¬ ê²°ê³¼:")
    print("="*70)
    
    passed = 0
    total = len(checks)
    
    for check_name, result in checks:
        status = "âœ… í†µê³¼" if result else "âŒ ì‹¤íŒ¨"
        print(f"   {check_name:<15}: {status}")
        if result:
            passed += 1
    
    print(f"\nğŸ“Š ì „ì²´ ê²°ê³¼: {passed}/{total} í†µê³¼")
    
    if passed == total:
        print("ğŸ‰ ëª¨ë“  ìš”êµ¬ì‚¬í•­ì„ ë§Œì¡±í•©ë‹ˆë‹¤!")
        print("âœ… íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤€ë¹„ ì™„ë£Œ")
        print("\nğŸš€ ë‹¤ìŒ ë‹¨ê³„:")
        print("   python3 run_pipeline.py")
        return True
    elif passed >= total - 2:
        print("âš ï¸ ëŒ€ë¶€ë¶„ì˜ ìš”êµ¬ì‚¬í•­ì„ ë§Œì¡±í•©ë‹ˆë‹¤")
        print("ğŸ’¡ ì¼ë¶€ ì œí•œì´ ìˆì„ ìˆ˜ ìˆì§€ë§Œ ì‹¤í–‰ ê°€ëŠ¥í•©ë‹ˆë‹¤")
        
        proceed = input("\nê³„ì† ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ").lower()
        if proceed == 'y':
            print("âœ… íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ì„ ì§„í–‰í•©ë‹ˆë‹¤")
            return True
        else:
            provide_solutions()
            return False
    else:
        print("âŒ í•„ìˆ˜ ìš”êµ¬ì‚¬í•­ì´ ë¶€ì¡±í•©ë‹ˆë‹¤")
        print("ğŸ’¡ ì•„ë˜ í•´ê²°ë°©ë²•ì„ ì°¸ê³ í•˜ì—¬ í™˜ê²½ì„ ì¤€ë¹„í•´ì£¼ì„¸ìš”")
        provide_solutions()
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1) 