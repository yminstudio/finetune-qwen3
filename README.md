# ğŸš€ Qwen3 Korean QLora Fine-tuning Pipeline

**Qwen3 ëª¨ë¸ì„ í•œêµ­ì–´ ê¸°ìˆ  Q&Aë¡œ íŒŒì¸íŠœë‹í•˜ê³  í”„ë¡œë•ì…˜ ì„œë¹„ìŠ¤ë¡œ ë°°í¬í•˜ëŠ” ì™„ì „ ìë™í™” íŒŒì´í”„ë¼ì¸**

## âœ¨ ì£¼ìš” íŠ¹ì§•

- ğŸ¤– **Qwen3-4B ëª¨ë¸** í•œêµ­ì–´ íŒŒì¸íŠœë‹
- âš¡ **QLora ê¸°ë²•** ì‚¬ìš©ìœ¼ë¡œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì  í•™ìŠµ
- ğŸ”„ **ì™„ì „ ìë™í™”** 6ë‹¨ê³„ íŒŒì´í”„ë¼ì¸
- ğŸš€ **í”„ë¡œë•ì…˜ ë°°í¬** Ollama + FastAPI
- ğŸ“Š **ì„±ëŠ¥ ìµœì í™”** GGUF ì–‘ìí™” ì ìš©
- ğŸ¯ **37.7 í† í°/ì´ˆ** ê³ ì† ì¶”ë¡ 

## ğŸ“‹ íŒŒì´í”„ë¼ì¸ ë‹¨ê³„

| ë‹¨ê³„ | ì„¤ëª… | ì†Œìš”ì‹œê°„ | ì¶œë ¥ë¬¼ |
|------|------|----------|--------|
| 0ï¸âƒ£ | ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­ ì²´í¬ | 5ë¶„ | í™˜ê²½ ê²€ì¦ |
| 1ï¸âƒ£ | ëª¨ë¸ ì¤€ë¹„ ë° í™˜ê²½ ì„¤ì • | 30ë¶„ | ë² ì´ìŠ¤ ëª¨ë¸ ìºì‹œ |
| 2ï¸âƒ£ | QLora íŒŒì¸íŠœë‹ ì‹¤í–‰ | 6ë¶„* | LoRA ì–´ëŒ‘í„° |
| 3ï¸âƒ£ | í•™ìŠµ ê²°ê³¼ ê²€ì¦ | 16ë¶„ | ì„±ëŠ¥ ë¦¬í¬íŠ¸ |
| 4ï¸âƒ£ | LoRA ì–´ëŒ‘í„° ë³‘í•© | 4ë¶„ | ë³‘í•©ëœ ëª¨ë¸ |
| 5ï¸âƒ£ | GGUF ë³€í™˜ ë° Ollama ë“±ë¡ | 7ë¶„ | ì–‘ìí™” ëª¨ë¸ |
| 6ï¸âƒ£ | ì„œë¹„ìŠ¤ ìµœì í™” ë° ë°°í¬ | 1ë¶„ | API ì„œë²„ |

> *GPU RTX A6000 ê¸°ì¤€, 5ê°œ ìƒ˜í”Œ ë°ì´í„°

## ğŸ› ï¸ ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­

### ìµœì†Œ ì‚¬ì–‘
- **GPU**: NVIDIA RTX A6000 (48GB VRAM) ë˜ëŠ” ë™ê¸‰
- **RAM**: 62GB
- **ë””ìŠ¤í¬**: 150GB ì—¬ìœ  ê³µê°„
- **OS**: Ubuntu 22.04 LTS
- **CUDA**: 12.8+

### í•„ìˆ˜ íŒ¨í‚¤ì§€
- Python 3.10+
- PyTorch 2.6.0+
- transformers 4.52.3+
- CUDA Toolkit

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

### 1. í™˜ê²½ ì„¤ì •
```bash
# ì €ì¥ì†Œ í´ë¡ 
git clone <your-repo-url>
cd qwen3-korean-finetune

# ì˜ì¡´ì„± ì„¤ì¹˜
pip install -r requirements.txt
```

### 2. ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
```bash
# ì›í´ë¦­ ì‹¤í–‰ (ì•½ 1ì‹œê°„)
python3 run_pipeline.py
```

### 3. ì„œë¹„ìŠ¤ ì‹œì‘
```bash
# Ollama ì„œë²„
ollama run qwen3-4b-finetune

# API ì„œë²„ (ë³„ë„ í„°ë¯¸ë„)
python3 api_server.py

# ë¸Œë¼ìš°ì €ì—ì„œ í™•ì¸
# http://localhost:8000/docs
```

## ğŸ“ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
qwen3-korean-finetune/
â”œâ”€â”€ ğŸ“„ run_pipeline.py          # ë©”ì¸ íŒŒì´í”„ë¼ì¸
â”œâ”€â”€ âš™ï¸ config.py                # ì„¤ì • íŒŒì¼
â”œâ”€â”€ ğŸ“‹ requirements.txt         # ì˜ì¡´ì„±
â”œâ”€â”€ ğŸŒ api_server.py            # FastAPI ì„œë²„
â”œâ”€â”€ ğŸ“Š dashboard.html           # ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ
â”œâ”€â”€ ğŸ”§ step0_check_requirements.py  # ì‹œìŠ¤í…œ ì²´í¬
â”œâ”€â”€ ğŸ”§ step1_setup_environment.py   # í™˜ê²½ ì„¤ì •
â”œâ”€â”€ ğŸ”§ step2_train_qlora.py         # íŒŒì¸íŠœë‹
â”œâ”€â”€ ğŸ”§ step3_validate_results.py    # ê²°ê³¼ ê²€ì¦
â”œâ”€â”€ ğŸ”§ step4_merge_adapters.py      # ëª¨ë¸ ë³‘í•©
â”œâ”€â”€ ğŸ”§ step5_convert_to_gguf.py     # GGUF ë³€í™˜
â”œâ”€â”€ ğŸ”§ step6_optimize_deployment.py # ë°°í¬ ìµœì í™”
â”œâ”€â”€ ğŸ“ data/                    # í•™ìŠµ ë°ì´í„°
â””â”€â”€ ğŸ“ models/                  # ì¶œë ¥ ëª¨ë¸
```

## ğŸ¯ ì„±ëŠ¥ ê²°ê³¼

### í•™ìŠµ ì„±ëŠ¥
- **íŒŒì¸íŠœë‹ íš¨ê³¼**: 97.3% ìœ ì‚¬ë„
- **í•™ìŠµ ì‹œê°„**: 6ë¶„ (100 ìŠ¤í…)
- **ë©”ëª¨ë¦¬ ì‚¬ìš©**: 2.8GB GPU

### ì¶”ë¡  ì„±ëŠ¥
- **ì§§ì€ ì‘ë‹µ**: 30.7 í† í°/ì´ˆ
- **ì¤‘ê°„ ì‘ë‹µ**: 38.2 í† í°/ì´ˆ  
- **ê¸´ ì‘ë‹µ**: 44.1 í† í°/ì´ˆ
- **í‰ê· **: 37.7 í† í°/ì´ˆ

### ëª¨ë¸ í¬ê¸°
- **ì›ë³¸ ëª¨ë¸**: 7.5GB
- **ì–‘ìí™” ëª¨ë¸**: 2.3GB (69% ì ˆì•½)

## ğŸ”§ ì»¤ìŠ¤í„°ë§ˆì´ì§•

### ëª¨ë¸ ë³€ê²½
```python
# config.py
MODEL_BASE_NAME = "Qwen3-8B"  # 4B â†’ 8Bë¡œ ë³€ê²½
```

### í•™ìŠµ ë°ì´í„° ì¶”ê°€
```bash
# data/ í´ë”ì— JSON íŒŒì¼ ì¶”ê°€
# í˜•ì‹: [{"messages": [{"role": "user", "content": "ì§ˆë¬¸"}, {"role": "assistant", "content": "ë‹µë³€"}]}]
```

### íŒŒë¼ë¯¸í„° ì¡°ì •
```python
# config.py
TRAINING_CONFIG = {
    "max_steps": 200,           # í•™ìŠµ ìŠ¤í… ì¦ê°€
    "learning_rate": 1e-4,      # í•™ìŠµë¥  ì¡°ì •
    "per_device_train_batch_size": 2  # ë°°ì¹˜ í¬ê¸° ì¦ê°€
}
```

## ğŸ“Š API ì‚¬ìš©ë²•

### FastAPI ì—”ë“œí¬ì¸íŠ¸

```bash
# ê±´ê°• ìƒíƒœ í™•ì¸
curl http://localhost:8000/health

# ì±„íŒ… API
curl -X POST http://localhost:8000/chat \
  -H "Content-Type: application/json" \
  -d '{
    "message": "íŒŒì´ì¬ ë¦¬ìŠ¤íŠ¸ ì»´í”„ë¦¬í—¨ì…˜ ì„¤ëª…í•´ì¤˜",
    "temperature": 0.7,
    "max_tokens": 200
  }'
```

### Ollama ì§ì ‘ ì‚¬ìš©
```bash
# ëŒ€í™”í˜• ëª¨ë“œ
ollama run qwen3-4b-finetune

# API ëª¨ë“œ
curl http://localhost:11434/api/generate \
  -d '{
    "model": "qwen3-4b-finetune",
    "prompt": "Docker ìµœì í™” íŒì€?",
    "stream": false
  }'
```

## ğŸ” íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### ë©”ëª¨ë¦¬ ë¶€ì¡±
```bash
# GPU ë©”ëª¨ë¦¬ í™•ì¸
nvidia-smi

# ë°°ì¹˜ í¬ê¸° ì¤„ì´ê¸°
# config.pyì—ì„œ per_device_train_batch_size = 1
```

### ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨
```bash
# ìºì‹œ ì •ë¦¬
rm -rf cache/ unsloth_compiled_cache/

# ë‹¤ì‹œ ì‹¤í–‰
python3 step1_setup_environment.py
```

## ğŸ“„ ë¼ì´ì„ ìŠ¤

MIT License