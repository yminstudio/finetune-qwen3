#!/usr/bin/env python3
"""
ğŸ”„ 4ë‹¨ê³„: LoRA ì–´ëŒ‘í„° ë³‘í•© (1ì‹œê°„)

ëª©ì :
- 4-bit Base + 16-bit LoRA â†’ 16-bit í†µí•© ëª¨ë¸ ìƒì„±
- ì¶”ë¡  ìµœì í™”ë¥¼ ìœ„í•œ ë‹¨ì¼ ëª¨ë¸ êµ¬ì¡° í™•ë³´
- í˜¸í™˜ì„± ë¬¸ì œ í•´ê²°
"""

import os
import sys
import time
import json
import torch
import gc
from pathlib import Path
from transformers import AutoTokenizer, AutoModelForCausalLM
from unsloth import FastLanguageModel
import psutil

# ì„¤ì • íŒŒì¼ import
from config import *

LOG_DIR = os.path.join(os.path.dirname(__file__), 'logs')
LOG_FILE = os.path.join(LOG_DIR, 'step4_merge_adapters.log')
os.makedirs(LOG_DIR, exist_ok=True)
def log_print(*args, **kwargs):
    print(*args, **kwargs)
    with open(LOG_FILE, 'a', encoding='utf-8') as f:
        print(*args, **kwargs, file=f)

def check_system_requirements():
    """ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­ í™•ì¸"""
    log_print("ğŸ’¾ ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­ í™•ì¸ ì¤‘...")
    
    # í˜„ì¬ ëª¨ë¸ì˜ ë©”ëª¨ë¦¬ ìš”êµ¬ì‚¬í•­ ê°€ì ¸ì˜¤ê¸°
    memory_req = get_memory_requirements()
    required_ram = memory_req["merge_ram"]
    required_disk = memory_req["disk_space"]
    
    # RAM í™•ì¸
    memory = psutil.virtual_memory()
    available_gb = memory.available / (1024**3)
    
    log_print(f"   ì‚¬ìš© ê°€ëŠ¥í•œ RAM: {available_gb:.1f}GB")
    log_print(f"   í•„ìš”í•œ RAM: {required_ram}GB ({MODEL_BASE_NAME} ëª¨ë¸)")
    
    # ëª¨ë¸ë³„ ë©”ëª¨ë¦¬ ìš”êµ¬ì‚¬í•­ ì²´í¬
    if available_gb < required_ram:
        log_print("   âš ï¸ RAM ë¶€ì¡± ê²½ê³ :")
        log_print(f"   ë³‘í•© ê³¼ì •ì—ì„œ {required_ram}GB+ ë©”ëª¨ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        log_print(f"   í˜„ì¬ ì‚¬ìš© ê°€ëŠ¥: {available_gb:.1f}GB")
        log_print("   âš ï¸ ë©”ëª¨ë¦¬ ë¶€ì¡±ìœ¼ë¡œ ì¸í•´ ë³‘í•©ì´ ì‹¤íŒ¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        log_print("   ğŸ’¡ ë‹¤ë¥¸ í”„ë¡œì„¸ìŠ¤ë¥¼ ì¢…ë£Œí•˜ê±°ë‚˜ ë” ì‘ì€ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì„¸ìš”.")
        # ìë™ìœ¼ë¡œ ê³„ì† ì§„í–‰ (ê²½ê³ ë§Œ í‘œì‹œ)
    
    # ë””ìŠ¤í¬ ê³µê°„ í™•ì¸ (ëª¨ë¸ë³„ ìš”êµ¬ì‚¬í•­)
    disk = psutil.disk_usage('/')
    free_gb = disk.free / (1024**3)
    
    log_print(f"   ì‚¬ìš© ê°€ëŠ¥í•œ ë””ìŠ¤í¬: {free_gb:.1f}GB")
    log_print(f"   í•„ìš”í•œ ë””ìŠ¤í¬: {required_disk}GB ({MODEL_BASE_NAME} ëª¨ë¸)")
    
    if free_gb < required_disk:
        log_print("   âŒ ë””ìŠ¤í¬ ê³µê°„ ë¶€ì¡±:")
        log_print(f"   ë³‘í•©ëœ ëª¨ë¸ ì €ì¥ì— {required_disk}GB+ í•„ìš”")
        log_print(f"   í˜„ì¬ ì‚¬ìš© ê°€ëŠ¥: {free_gb:.1f}GB")
        return False
    
    log_print("   âœ… ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­ í™•ì¸ ì™„ë£Œ")
    return True

def load_base_model_16bit():
    """ë² ì´ìŠ¤ ëª¨ë¸ì„ 16-bitë¡œ ë¡œë“œ"""
    log_print(f"ğŸ¤– ë² ì´ìŠ¤ ëª¨ë¸ 16-bit ë¡œë“œ ì¤‘: {MODEL_NAME}")
    
    # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    gc.collect()
    
    start_time = time.time()
    
    try:
        # 16-bitë¡œ ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ (ì–‘ìí™” ì—†ìŒ)
        model = AutoModelForCausalLM.from_pretrained(
            MODEL_NAME,
            torch_dtype=torch.bfloat16,
            device_map="auto",
            trust_remote_code=True
        )
        
        tokenizer = AutoTokenizer.from_pretrained(
            MODEL_NAME,
            trust_remote_code=True
        )
        
        load_time = time.time() - start_time
        log_print(f"   âœ… ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ ({load_time:.1f}ì´ˆ)")
        
        # ëª¨ë¸ í¬ê¸° ì •ë³´
        total_params = sum(p.numel() for p in model.parameters())
        memory_gb = sum(p.numel() * p.element_size() for p in model.parameters()) / (1024**3)
        
        log_print(f"   ëª¨ë¸ íŒŒë¼ë¯¸í„°: {total_params:,}")
        log_print(f"   ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory_gb:.1f}GB")
        
        return model, tokenizer
        
    except Exception as e:
        log_print(f"âŒ ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None, None

def load_lora_adapters():
    """LoRA ì–´ëŒ‘í„° ë¡œë“œ"""
    log_print("ğŸ”§ LoRA ì–´ëŒ‘í„° ë¡œë“œ ì¤‘...")
    
    lora_adapter_path = OUTPUT_DIR / "qwen3_lora_adapters"
    
    if not lora_adapter_path.exists():
        log_print("âŒ LoRA ì–´ëŒ‘í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        log_print("   ë¨¼ì € 2ë‹¨ê³„ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”: python3 step2_train_qlora.py")
        return None
    
    # ì–´ëŒ‘í„° íŒŒì¼ í™•ì¸
    adapter_files = list(lora_adapter_path.glob("adapter_*.safetensors"))
    if not adapter_files:
        adapter_files = list(lora_adapter_path.glob("adapter_*.bin"))
    
    if not adapter_files:
        log_print("âŒ ì–´ëŒ‘í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None
    
    adapter_file = adapter_files[0]
    adapter_size_gb = adapter_file.stat().st_size / (1024**3)
    
    log_print(f"   ì–´ëŒ‘í„° íŒŒì¼: {adapter_file.name}")
    log_print(f"   ì–´ëŒ‘í„° í¬ê¸°: {adapter_size_gb:.2f}GB")
    
    return lora_adapter_path

def merge_lora_with_base(model, tokenizer, lora_adapter_path):
    """LoRA ì–´ëŒ‘í„°ë¥¼ ë² ì´ìŠ¤ ëª¨ë¸ì— ë³‘í•©"""
    log_print("ğŸ”„ LoRA ì–´ëŒ‘í„° ë³‘í•© ì¤‘...")
    
    start_time = time.time()
    
    try:
        from peft import PeftModel
        
        # PEFT ëª¨ë¸ë¡œ ì–´ëŒ‘í„° ë¡œë“œ
        log_print("   PEFT ëª¨ë¸ ìƒì„± ì¤‘...")
        peft_model = PeftModel.from_pretrained(
            model,
            str(lora_adapter_path),
            torch_dtype=torch.bfloat16
        )
        
        log_print("   ì–´ëŒ‘í„° ë³‘í•© ì‹¤í–‰ ì¤‘...")
        # ë³‘í•© ì‹¤í–‰ - ì´ ê³¼ì •ì—ì„œ ë§ì€ ë©”ëª¨ë¦¬ ì‚¬ìš©
        merged_model = peft_model.merge_and_unload()
        
        merge_time = time.time() - start_time
        log_print(f"   âœ… ë³‘í•© ì™„ë£Œ ({merge_time:.1f}ì´ˆ)")
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        del peft_model
        torch.cuda.empty_cache() if torch.cuda.is_available() else None
        gc.collect()
        
        return merged_model
        
    except Exception as e:
        log_print(f"âŒ ë³‘í•© ì‹¤íŒ¨: {e}")
        
        # ëŒ€ì•ˆ ë°©ë²• ì‹œë„ (Unsloth ì‚¬ìš©)
        log_print("   ëŒ€ì•ˆ ë°©ë²• ì‹œë„ ì¤‘...")
        try:
            # Unslothë¡œ ë‹¤ì‹œ ë¡œë“œ í›„ ë³‘í•©
            model_4bit, tokenizer = FastLanguageModel.from_pretrained(
                model_name=MODEL_NAME,
                max_seq_length=MAX_SEQ_LENGTH,
                dtype=None,
                load_in_4bit=True,
            )
            
            # LoRA ì–´ëŒ‘í„° ì¶”ê°€
            model_4bit = FastLanguageModel.get_peft_model(
                model_4bit,
                **QLORA_CONFIG
            )
            
            # ì–´ëŒ‘í„° ê°€ì¤‘ì¹˜ ë¡œë“œ
            model_4bit.load_state_dict(
                torch.load(lora_adapter_path / "adapter_model.bin"), 
                strict=False
            )
            
            # 16-bitë¡œ ë³‘í•©
            merged_model = model_4bit.merge_and_unload()
            
            log_print("   âœ… ëŒ€ì•ˆ ë°©ë²•ìœ¼ë¡œ ë³‘í•© ì™„ë£Œ")
            return merged_model
            
        except Exception as e2:
            log_print(f"âŒ ëŒ€ì•ˆ ë°©ë²•ë„ ì‹¤íŒ¨: {e2}")
            return None

def verify_merged_model(merged_model, tokenizer):
    """ë³‘í•©ëœ ëª¨ë¸ ê²€ì¦"""
    log_print("ğŸ§ª ë³‘í•©ëœ ëª¨ë¸ ê²€ì¦ ì¤‘...")
    
    # ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ìƒì„± í…ŒìŠ¤íŠ¸
    test_prompts = [
        "ì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ”",
        "íŒŒì´ì¬ì—ì„œ ë¦¬ìŠ¤íŠ¸ ì»´í”„ë¦¬í—¨ì…˜ì´ë€",
        "ë¨¸ì‹ ëŸ¬ë‹ì˜ ê¸°ë³¸ ê°œë…ì€"
    ]
    
    for i, prompt in enumerate(test_prompts):
        log_print(f"\n   í…ŒìŠ¤íŠ¸ {i+1}/3: {prompt}")
        
        try:
            inputs = tokenizer(prompt, return_tensors="pt").to(merged_model.device)
            
            with torch.no_grad():
                outputs = merged_model.generate(
                    **inputs,
                    max_new_tokens=50,
                    temperature=0.7,
                    do_sample=True,
                    pad_token_id=tokenizer.eos_token_id
                )
            
            response = tokenizer.decode(outputs[0], skip_special_tokens=True)
            log_print(f"   ì‘ë‹µ: {response[len(prompt):].strip()[:100]}...")
            
        except Exception as e:
            log_print(f"   âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            return False
    
    log_print("   âœ… ë³‘í•©ëœ ëª¨ë¸ ê²€ì¦ ì™„ë£Œ")
    return True

def save_merged_model(merged_model, tokenizer):
    """ë³‘í•©ëœ ëª¨ë¸ ì €ì¥"""
    log_print("ğŸ’¾ ë³‘í•©ëœ ëª¨ë¸ ì €ì¥ ì¤‘...")
    
    # ì €ì¥ ê²½ë¡œ ì„¤ì •
    merged_model_dir = OUTPUT_DIR / "qwen3_finetune_merged"
    merged_model_dir.mkdir(exist_ok=True)
    
    start_time = time.time()
    
    try:
        # ëª¨ë¸ ì €ì¥
        log_print("   ëª¨ë¸ ê°€ì¤‘ì¹˜ ì €ì¥ ì¤‘...")
        merged_model.save_pretrained(
            str(merged_model_dir),
            safe_serialization=True,  # safetensors í˜•ì‹ ì‚¬ìš©
            max_shard_size="5GB"      # íŒŒì¼ í¬ê¸° ì œí•œ
        )
        
        # í† í¬ë‚˜ì´ì € ì €ì¥
        log_print("   í† í¬ë‚˜ì´ì € ì €ì¥ ì¤‘...")
        tokenizer.save_pretrained(str(merged_model_dir))
        
        save_time = time.time() - start_time
        log_print(f"   âœ… ì €ì¥ ì™„ë£Œ ({save_time:.1f}ì´ˆ)")
        
        # ì €ì¥ëœ ëª¨ë¸ í¬ê¸° í™•ì¸
        total_size = sum(f.stat().st_size for f in merged_model_dir.rglob('*') if f.is_file())
        size_gb = total_size / (1024**3)
        
        log_print(f"   ì €ì¥ ìœ„ì¹˜: {merged_model_dir}")
        log_print(f"   ëª¨ë¸ í¬ê¸°: {size_gb:.1f}GB")
        
        # ë©”íƒ€ë°ì´í„° ì €ì¥
        metadata = {
            "model_name": MODEL_NAME,
            "merged_timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "original_lora_config": QLORA_CONFIG,
            "merged_model_size_gb": size_gb,
            "model_type": "merged_16bit",
            "description": "LoRA ì–´ëŒ‘í„°ê°€ ë² ì´ìŠ¤ ëª¨ë¸ì— ë³‘í•©ëœ 16-bit ëª¨ë¸"
        }
        
        metadata_file = merged_model_dir / "merge_info.json"
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
        
        log_print(f"   ë©”íƒ€ë°ì´í„° ì €ì¥: {metadata_file}")
        
        return merged_model_dir
        
    except Exception as e:
        log_print(f"âŒ ì €ì¥ ì‹¤íŒ¨: {e}")
        return None

def cleanup_intermediate_files():
    """ì¤‘ê°„ íŒŒì¼ ì •ë¦¬"""
    log_print("ğŸ§¹ ì¤‘ê°„ íŒŒì¼ ì •ë¦¬ ì¤‘...")
    
    # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    
    # ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ ì •ë¦¬
    gc.collect()
    
    log_print("   ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    log_print("ğŸ”„ 4ë‹¨ê³„: LoRA ì–´ëŒ‘í„° ë³‘í•© ì‹œì‘")
    log_print("=" * 50)
    
    try:
        # 1. ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­ í™•ì¸
        if not check_system_requirements():
            log_print("âŒ ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­ ë¶€ì¡±")
            return False
        
        # 2. LoRA ì–´ëŒ‘í„° í™•ì¸
        lora_adapter_path = load_lora_adapters()
        if lora_adapter_path is None:
            return False
        
        # 3. ë² ì´ìŠ¤ ëª¨ë¸ 16-bit ë¡œë“œ
        model, tokenizer = load_base_model_16bit()
        if model is None:
            return False
        
        # 4. LoRA ì–´ëŒ‘í„° ë³‘í•©
        merged_model = merge_lora_with_base(model, tokenizer, lora_adapter_path)
        if merged_model is None:
            return False
        
        # 5. ë³‘í•©ëœ ëª¨ë¸ ê²€ì¦
        if not verify_merged_model(merged_model, tokenizer):
            return False
        
        # 6. ë³‘í•©ëœ ëª¨ë¸ ì €ì¥
        merged_model_dir = save_merged_model(merged_model, tokenizer)
        if merged_model_dir is None:
            return False
        
        # 7. ì •ë¦¬
        cleanup_intermediate_files()
        
        log_print("=" * 50)
        log_print("âœ… 4ë‹¨ê³„ ì™„ë£Œ!")
        log_print("ğŸ“‹ ê²°ê³¼:")
        log_print(f"   ë³‘í•©ëœ ëª¨ë¸: {merged_model_dir}")
        log_print(f"   íŒŒì¸íŠœë‹ íš¨ê³¼: LoRA ì–´ëŒ‘í„°ê°€ ë² ì´ìŠ¤ ëª¨ë¸ì— ì™„ì „íˆ ë³‘í•©ë¨")
        log_print(f"   ëª¨ë¸ í˜•ì‹: í‘œì¤€ 16-bit transformers ëª¨ë¸")
        log_print("\nğŸ“‹ ë‹¤ìŒ ë‹¨ê³„ ì•ˆë‚´:")
        log_print("   - 5ë‹¨ê³„ ì‹¤í–‰: python3 step5_convert_to_gguf.py")
        log_print("   - ë³‘í•©ëœ ëª¨ë¸ì„ GGUFë¡œ ë³€í™˜í•˜ì—¬ Ollamaì—ì„œ ì‚¬ìš©")
        
        return True
        
    except Exception as e:
        log_print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1) 