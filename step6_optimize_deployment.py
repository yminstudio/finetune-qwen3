#!/usr/bin/env python3
"""
ğŸš€ 6ë‹¨ê³„: ì„œë¹„ìŠ¤ ìµœì í™” ë° ë°°í¬ (1ì‹œê°„)

ëª©ì :
- í”„ë¡œë•ì…˜ í™˜ê²½ì— ì í•©í•œ ì„±ëŠ¥ ë‹¬ì„±
- API ì„œë²„ êµ¬ì¶• ë° ëª¨ë‹ˆí„°ë§ ì„¤ì •
- í™•ì¥ì„± ë° ì•ˆì •ì„± í™•ë³´
"""

import os
import sys
import time
import json
import subprocess
import threading
import requests
from pathlib import Path
import psutil
import GPUtil
from config import PROJECT_ROOT

# ì„¤ì • íŒŒì¼ import
from config import *
from config import OLLAMA_CONFIG, GGUF_CONFIG, LLAMA_CPP_DIR, OUTPUT_DIR

def check_ollama_service():
    """Ollama ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸"""
    print("ğŸ” Ollama ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸ ì¤‘...")
    
    try:
        # Ollama í”„ë¡œì„¸ìŠ¤ í™•ì¸
        ollama_processes = []
        for proc in psutil.process_iter(['pid', 'name', 'cmdline']):
            if 'ollama' in proc.info['name'].lower():
                ollama_processes.append(proc.info)
        
        if ollama_processes:
            print(f"   âœ… Ollama í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰ ì¤‘: {len(ollama_processes)}ê°œ")
        else:
            print("   âš ï¸ Ollama í”„ë¡œì„¸ìŠ¤ê°€ ì‹¤í–‰ë˜ì§€ ì•ŠìŒ")
            return False
        
        # API ì„œë²„ ì‘ë‹µ í™•ì¸
        try:
            response = requests.get("http://localhost:11434/api/tags", timeout=5)
            if response.status_code == 200:
                models = response.json().get('models', [])
                print(f"   âœ… API ì„œë²„ ì‘ë‹µ OK: {len(models)}ê°œ ëª¨ë¸")
                return True
            else:
                print(f"   âŒ API ì„œë²„ ì˜¤ë¥˜: {response.status_code}")
                return False
        except requests.exceptions.RequestException as e:
            print(f"   âŒ API ì„œë²„ ì—°ê²° ì‹¤íŒ¨: {e}")
            return False
            
    except Exception as e:
        print(f"   âŒ ì„œë¹„ìŠ¤ í™•ì¸ ì‹¤íŒ¨: {e}")
        return False

def start_ollama_service():
    """Ollama ì„œë¹„ìŠ¤ ì‹œì‘"""
    print("ğŸš€ Ollama ì„œë¹„ìŠ¤ ì‹œì‘ ì¤‘...")
    
    try:
        # Ollama ì„œë²„ ì‹œì‘ (ë°±ê·¸ë¼ìš´ë“œ)
        subprocess.Popen(
            ["ollama", "serve"],
            stdout=subprocess.DEVNULL,
            stderr=subprocess.DEVNULL
        )
        
        # ì„œë¹„ìŠ¤ ì‹œì‘ ëŒ€ê¸°
        print("   ì„œë¹„ìŠ¤ ì‹œì‘ ëŒ€ê¸° ì¤‘...")
        for i in range(30):  # 30ì´ˆ ëŒ€ê¸°
            time.sleep(1)
            try:
                response = requests.get("http://localhost:11434/api/tags", timeout=2)
                if response.status_code == 200:
                    print("   âœ… Ollama ì„œë¹„ìŠ¤ ì‹œì‘ ì™„ë£Œ")
                    return True
            except:
                continue
        
        print("   âŒ Ollama ì„œë¹„ìŠ¤ ì‹œì‘ ì‹¤íŒ¨")
        return False
        
    except Exception as e:
        print(f"   âŒ ì„œë¹„ìŠ¤ ì‹œì‘ ì˜¤ë¥˜: {e}")
        return False

def optimize_ollama_performance():
    """Ollama ì„±ëŠ¥ ìµœì í™”"""
    print("âš¡ Ollama ì„±ëŠ¥ ìµœì í™” ì¤‘...")
    
    # í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
    optimization_env = {
        "OLLAMA_NUM_PARALLEL": "2",          # ë³‘ë ¬ ìš”ì²­ ìˆ˜
        "OLLAMA_MAX_LOADED_MODELS": "1",     # ë©”ëª¨ë¦¬ì— ë¡œë“œí•  ìµœëŒ€ ëª¨ë¸ ìˆ˜
        "OLLAMA_FLASH_ATTENTION": "1",       # Flash Attention í™œì„±í™”
        "OLLAMA_GPU_OVERHEAD": "0.1",        # GPU ì˜¤ë²„í—¤ë“œ ì„¤ì •
    }
    
    # í˜„ì¬ í™˜ê²½ì— ì ìš©
    for key, value in optimization_env.items():
        os.environ[key] = value
        print(f"   {key}={value}")
    
    # ì„¤ì • íŒŒì¼ ìƒì„±
    ollama_config = {
        "experimental": {
            "numa": True
        },
        "gpu": {
            "enabled": True,
            "memory_fraction": 0.9
        },
        "server": {
            "host": "0.0.0.0",
            "port": 11434,
            "timeout": "5m"
        }
    }
    
    config_dir = Path.home() / ".ollama"
    config_dir.mkdir(exist_ok=True)
    
    config_file = config_dir / "config.json"
    with open(config_file, 'w') as f:
        json.dump(ollama_config, f, indent=2)
    
    print(f"   âœ… ì„¤ì • íŒŒì¼ ìƒì„±: {config_file}")

def benchmark_model_performance():
    """ëª¨ë¸ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
    print("ğŸ“Š ëª¨ë¸ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì¤‘...")
    
    model_name = OLLAMA_CONFIG['model_name']
    
    # ë‹¤ì–‘í•œ ê¸¸ì´ì˜ í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸
    test_cases = [
        {
            "name": "ì§§ì€ ì‘ë‹µ",
            "prompt": "ì•ˆë…•í•˜ì„¸ìš”!",
            "expected_tokens": 20
        },
        {
            "name": "ì¤‘ê°„ ì‘ë‹µ", 
            "prompt": "íŒŒì´ì¬ ë¦¬ìŠ¤íŠ¸ ì»´í”„ë¦¬í—¨ì…˜ì˜ ì¥ì ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
            "expected_tokens": 150
        },
        {
            "name": "ê¸´ ì‘ë‹µ",
            "prompt": "ë¨¸ì‹ ëŸ¬ë‹ í”„ë¡œì íŠ¸ì˜ ì „ì²´ ì›Œí¬í”Œë¡œìš°ë¥¼ ë‹¨ê³„ë³„ë¡œ ìì„¸íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
            "expected_tokens": 300
        }
    ]
    
    benchmark_results = []
    
    for test_case in test_cases:
        print(f"\n   ğŸ§ª {test_case['name']} í…ŒìŠ¤íŠ¸...")
        
        # ì—¬ëŸ¬ ë²ˆ ì‹¤í–‰í•˜ì—¬ í‰ê·  êµ¬í•˜ê¸°
        times = []
        token_counts = []
        
        for i in range(3):
            start_time = time.time()
            
            try:
                # API í˜¸ì¶œ
                response = requests.post(
                    "http://localhost:11434/api/generate",
                    json={
                        "model": model_name,
                        "prompt": test_case["prompt"],
                        "stream": False,
                        "options": {
                            "temperature": 0.7,
                            "num_predict": test_case["expected_tokens"]
                        }
                    },
                    timeout=60
                )
                
                elapsed_time = time.time() - start_time
                
                if response.status_code == 200:
                    result = response.json()
                    generated_text = result.get('response', '')
                    
                    # í† í° ìˆ˜ ì¶”ì • (ê°„ë‹¨í•œ ë°©ë²•)
                    estimated_tokens = len(generated_text.split()) * 1.3
                    
                    times.append(elapsed_time)
                    token_counts.append(estimated_tokens)
                    
                    print(f"      ì‹œë„ {i+1}: {elapsed_time:.1f}ì´ˆ, ~{estimated_tokens:.0f} í† í°")
                
            except Exception as e:
                print(f"      ì‹œë„ {i+1} ì‹¤íŒ¨: {e}")
        
        if times:
            avg_time = sum(times) / len(times)
            avg_tokens = sum(token_counts) / len(token_counts)
            tokens_per_sec = avg_tokens / avg_time if avg_time > 0 else 0
            
            result = {
                "test_name": test_case["name"],
                "avg_time": avg_time,
                "avg_tokens": avg_tokens,
                "tokens_per_sec": tokens_per_sec
            }
            
            benchmark_results.append(result)
            
            print(f"   í‰ê· : {avg_time:.1f}ì´ˆ, {avg_tokens:.0f} í† í°, {tokens_per_sec:.1f} í† í°/ì´ˆ")
    
    return benchmark_results

def stress_test_concurrent_requests():
    """ë™ì‹œ ìš”ì²­ ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸"""
    print("ğŸ”¥ ë™ì‹œ ìš”ì²­ ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ ì¤‘...")
    
    model_name = OLLAMA_CONFIG['model_name']
    test_prompt = "Pythonì—ì„œ ë¦¬ìŠ¤íŠ¸ì™€ ë”•ì…”ë„ˆë¦¬ì˜ ì°¨ì´ì ì€?"
    
    def make_request(request_id):
        """ë‹¨ì¼ ìš”ì²­ ì‹¤í–‰"""
        try:
            start_time = time.time()
            response = requests.post(
                "http://localhost:11434/api/generate",
                json={
                    "model": model_name,
                    "prompt": test_prompt,
                    "stream": False,
                    "options": {"num_predict": 100}
                },
                timeout=120
            )
            elapsed_time = time.time() - start_time
            
            return {
                "request_id": request_id,
                "success": response.status_code == 200,
                "time": elapsed_time,
                "status_code": response.status_code
            }
        except Exception as e:
            return {
                "request_id": request_id,
                "success": False,
                "time": 0,
                "error": str(e)
            }
    
    # ë™ì‹œ ìš”ì²­ ìˆ˜ í…ŒìŠ¤íŠ¸
    concurrent_levels = [1, 2, 3]
    stress_results = []
    
    for concurrent_count in concurrent_levels:
        print(f"\n   ë™ì‹œ ìš”ì²­ {concurrent_count}ê°œ í…ŒìŠ¤íŠ¸...")
        
        # ìŠ¤ë ˆë“œë¡œ ë™ì‹œ ìš”ì²­ ì‹¤í–‰
        threads = []
        results = []
        
        start_time = time.time()
        
        for i in range(concurrent_count):
            thread = threading.Thread(
                target=lambda i=i: results.append(make_request(i))
            )
            threads.append(thread)
            thread.start()
        
        # ëª¨ë“  ìŠ¤ë ˆë“œ ì™„ë£Œ ëŒ€ê¸°
        for thread in threads:
            thread.join()
        
        total_time = time.time() - start_time
        
        # ê²°ê³¼ ë¶„ì„
        successful_requests = [r for r in results if r.get('success', False)]
        failed_requests = [r for r in results if not r.get('success', False)]
        
        if successful_requests:
            avg_response_time = sum(r['time'] for r in successful_requests) / len(successful_requests)
            success_rate = len(successful_requests) / len(results) * 100
        else:
            avg_response_time = 0
            success_rate = 0
        
        stress_result = {
            "concurrent_requests": concurrent_count,
            "total_time": total_time,
            "success_rate": success_rate,
            "avg_response_time": avg_response_time,
            "successful_count": len(successful_requests),
            "failed_count": len(failed_requests)
        }
        
        stress_results.append(stress_result)
        
        print(f"      ì´ ì‹œê°„: {total_time:.1f}ì´ˆ")
        print(f"      ì„±ê³µë¥ : {success_rate:.1f}%")
        print(f"      í‰ê·  ì‘ë‹µì‹œê°„: {avg_response_time:.1f}ì´ˆ")
        
        # ë‹¤ìŒ í…ŒìŠ¤íŠ¸ ì „ ì ì‹œ ëŒ€ê¸°
        time.sleep(5)
    
    return stress_results

def create_api_wrapper():
    """FastAPI ë˜í¼ ìƒì„±"""
    print("ğŸŒ API ë˜í¼ ìƒì„± ì¤‘...")
    
    api_wrapper_code = '''
#!/usr/bin/env python3
"""
FastAPI ë˜í¼ - Ollama ëª¨ë¸ì„ ìœ„í•œ RESTful API
"""

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import requests
import time
from typing import Optional

app = FastAPI(title="Qwen3 Korean API", version="1.0.0")

class ChatRequest(BaseModel):
    message: str
    temperature: Optional[float] = 0.7
    max_tokens: Optional[int] = 512

class ChatResponse(BaseModel):
    response: str
    processing_time: float
    model: str

@app.get("/")
async def root():
    return {"message": "Qwen3 Korean API Server", "status": "running"}

@app.get("/health")
async def health_check():
    try:
        response = requests.get("http://localhost:11434/api/tags", timeout=5)
        if response.status_code == 200:
            return {"status": "healthy", "ollama": "running"}
        else:
            raise HTTPException(status_code=503, detail="Ollama service unavailable")
    except:
        raise HTTPException(status_code=503, detail="Ollama service unavailable")

@app.post("/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    start_time = time.time()
    
    try:
        response = requests.post(
            "http://localhost:11434/api/generate",
            json={
                "model": "qwen3-4b-finetune",
                "prompt": request.message,
                "stream": False,
                "options": {
                    "temperature": request.temperature,
                    "num_predict": request.max_tokens
                }
            },
            timeout=120
        )
        
        if response.status_code == 200:
            result = response.json()
            processing_time = time.time() - start_time
            
            return ChatResponse(
                response=result.get('response', ''),
                processing_time=processing_time,
                model="qwen3-4b-finetune"
            )
        else:
            raise HTTPException(status_code=500, detail="Model generation failed")
            
    except requests.exceptions.Timeout:
        raise HTTPException(status_code=504, detail="Request timeout")
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
'''
    
    api_file = PROJECT_ROOT / "api_server.py"
    with open(api_file, 'w', encoding='utf-8') as f:
        f.write(api_wrapper_code)
    
    print(f"   âœ… API ë˜í¼ ìƒì„±: {api_file}")
    print("   ì‚¬ìš©ë²•: python3 api_server.py")
    
    return api_file

def create_monitoring_dashboard():
    """ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ HTML ìƒì„±"""
    print("ğŸ“Š ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ ìƒì„± ì¤‘...")
    
    dashboard_html = '''
<!DOCTYPE html>
<html>
<head>
    <title>Qwen3 Korean Model Monitoring</title>
    <meta charset="UTF-8">
    <style>
        body { font-family: Arial, sans-serif; margin: 20px; }
        .container { max-width: 1200px; margin: 0 auto; }
        .card { border: 1px solid #ddd; border-radius: 8px; padding: 20px; margin: 10px 0; }
        .status-ok { color: green; }
        .status-error { color: red; }
        .metric { display: inline-block; margin: 10px; padding: 10px; border-radius: 4px; background: #f5f5f5; }
        button { padding: 10px 20px; margin: 5px; cursor: pointer; }
        #chat-container { height: 400px; overflow-y: scroll; border: 1px solid #ccc; padding: 10px; }
        .chat-message { margin: 10px 0; }
        .user-message { text-align: right; color: blue; }
        .bot-message { text-align: left; color: green; }
    </style>
</head>
<body>
    <div class="container">
        <h1>ğŸ¤– Qwen3 Korean Model Dashboard</h1>
        
        <div class="card">
            <h2>ì„œë¹„ìŠ¤ ìƒíƒœ</h2>
            <div id="service-status">Loading...</div>
            <button onclick="checkStatus()">ìƒíƒœ ìƒˆë¡œê³ ì¹¨</button>
        </div>
        
        <div class="card">
            <h2>ì„±ëŠ¥ ë©”íŠ¸ë¦­</h2>
            <div id="performance-metrics">
                <div class="metric">ì‘ë‹µ ì‹œê°„: <span id="response-time">-</span></div>
                <div class="metric">ì²˜ë¦¬ëŸ‰: <span id="throughput">-</span></div>
                <div class="metric">ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: <span id="memory-usage">-</span></div>
            </div>
        </div>
        
        <div class="card">
            <h2>í…ŒìŠ¤íŠ¸ ì±„íŒ…</h2>
            <div id="chat-container"></div>
            <input type="text" id="chat-input" placeholder="ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”..." style="width: 70%;">
            <button onclick="sendMessage()">ì „ì†¡</button>
        </div>
    </div>

    <script>
        async function checkStatus() {
            try {
                const response = await fetch('/health');
                const data = await response.json();
                document.getElementById('service-status').innerHTML = 
                    '<span class="status-ok">âœ… ì„œë¹„ìŠ¤ ì •ìƒ</span>';
            } catch (error) {
                document.getElementById('service-status').innerHTML = 
                    '<span class="status-error">âŒ ì„œë¹„ìŠ¤ ì˜¤ë¥˜</span>';
            }
        }
        
        async function sendMessage() {
            const input = document.getElementById('chat-input');
            const message = input.value.trim();
            if (!message) return;
            
            const chatContainer = document.getElementById('chat-container');
            
            // ì‚¬ìš©ì ë©”ì‹œì§€ í‘œì‹œ
            chatContainer.innerHTML += `<div class="chat-message user-message">ğŸ‘¤ ${message}</div>`;
            input.value = '';
            
            // ë´‡ ì‘ë‹µ ë¡œë”©
            chatContainer.innerHTML += `<div class="chat-message bot-message">ğŸ¤– ìƒê° ì¤‘...</div>`;
            
            try {
                const startTime = Date.now();
                const response = await fetch('/chat', {
                    method: 'POST',
                    headers: {'Content-Type': 'application/json'},
                    body: JSON.stringify({message: message})
                });
                
                const data = await response.json();
                const endTime = Date.now();
                
                // ë¡œë”© ë©”ì‹œì§€ ì œê±° í›„ ì‹¤ì œ ì‘ë‹µ í‘œì‹œ
                const messages = chatContainer.querySelectorAll('.chat-message');
                messages[messages.length - 1].innerHTML = 
                    `ğŸ¤– ${data.response} <small>(${(endTime - startTime)}ms)</small>`;
                
                // ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
                document.getElementById('response-time').textContent = `${data.processing_time.toFixed(1)}ì´ˆ`;
                
            } catch (error) {
                const messages = chatContainer.querySelectorAll('.chat-message');
                messages[messages.length - 1].innerHTML = 'ğŸ¤– âŒ ì˜¤ë¥˜ ë°œìƒ';
            }
            
            chatContainer.scrollTop = chatContainer.scrollHeight;
        }
        
        // Enter í‚¤ë¡œ ë©”ì‹œì§€ ì „ì†¡
        document.getElementById('chat-input').addEventListener('keypress', function(e) {
            if (e.key === 'Enter') {
                sendMessage();
            }
        });
        
        // í˜ì´ì§€ ë¡œë“œ ì‹œ ìƒíƒœ í™•ì¸
        checkStatus();
        setInterval(checkStatus, 30000); // 30ì´ˆë§ˆë‹¤ ìƒíƒœ í™•ì¸
    </script>
</body>
</html>
'''
    
    dashboard_file = PROJECT_ROOT / "dashboard.html"
    with open(dashboard_file, 'w', encoding='utf-8') as f:
        f.write(dashboard_html)
    
    print(f"   âœ… ëŒ€ì‹œë³´ë“œ ìƒì„±: {dashboard_file}")
    print("   ë¸Œë¼ìš°ì €ì—ì„œ íŒŒì¼ì„ ì—´ì–´ ëª¨ë‹ˆí„°ë§ ê°€ëŠ¥")
    
    return dashboard_file

def save_deployment_report(benchmark_results, stress_results):
    """ë°°í¬ ë³´ê³ ì„œ ì €ì¥"""
    report = {
        "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
        "model_name": OLLAMA_CONFIG['model_name'],
        "performance_benchmark": benchmark_results,
        "stress_test": stress_results,
        "system_info": {
            "cpu_count": os.cpu_count(),
            "memory_gb": psutil.virtual_memory().total / (1024**3),
            "gpu_info": [gpu.name for gpu in GPUtil.getGPUs()] if GPUtil.getGPUs() else []
        },
        "deployment_config": {
            "ollama_config": OLLAMA_CONFIG,
            "api_config": API_CONFIG
        }
    }
    
    report_file = OUTPUT_DIR / "deployment_report.json"
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ’¾ ë°°í¬ ë³´ê³ ì„œ ì €ì¥: {report_file}")

def register_model_to_ollama():
    """Ollamaì— gguf ëª¨ë¸ ë“±ë¡ (Modelfile ìë™ ìƒì„±)"""
    print("ğŸ“ Ollamaì— ëª¨ë¸ ë“±ë¡ ì¤‘...")
    # ì‹¤ì œ ì–‘ìí™”ëœ GGUF íŒŒì¼ ê²½ë¡œë¡œ ìˆ˜ì •
    gguf_path = PROJECT_ROOT / "models" / f"{MODEL_BASE_NAME.lower()}-finetune-q4_k_m.gguf"
    modelfile_path = gguf_path.parent / "modelfile"
    
    if not gguf_path.exists():
        print(f"âŒ GGUF íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {gguf_path}")
        return False
    
    # Modelfile ìƒì„±
    with open(modelfile_path, "w", encoding="utf-8") as f:
        f.write(f"FROM {gguf_path}\n")
    
    print(f"   âœ… Modelfile ìƒì„±: {modelfile_path}")
    
    try:
        # ollama create ëª…ë ¹ì–´ ì‹¤í–‰ (Modelfile ì‚¬ìš©)
        import subprocess
        model_name = f"{MODEL_BASE_NAME.lower()}-finetune"
        result = subprocess.run([
            "ollama", "create", model_name, "-f", str(modelfile_path)
        ], capture_output=True, text=True)
        
        if result.returncode == 0:
            print(f"   âœ… ëª¨ë¸ ë“±ë¡ ì„±ê³µ: {model_name}")
            return True
        else:
            print(f"   âŒ ëª¨ë¸ ë“±ë¡ ì‹¤íŒ¨: {result.stderr}")
            return False
    except Exception as e:
        print(f"   âŒ ëª¨ë¸ ë“±ë¡ ì¤‘ ì˜¤ë¥˜: {e}")
        return False

def test_ollama_inference():
    """Ollamaì— ì§ˆë¬¸ì„ ë³´ë‚´ê³  ì‘ë‹µì„ ë°›ëŠ” í…ŒìŠ¤íŠ¸"""
    print("ğŸ¤– Ollama ì§ˆë¬¸-ì‘ë‹µ í…ŒìŠ¤íŠ¸ ì¤‘...")
    import requests
    test_questions = [
        "íŒŒì´ì¬ì—ì„œ ë¦¬ìŠ¤íŠ¸ì™€ ë”•ì…”ë„ˆë¦¬ì˜ ì°¨ì´ì ì€?",
        "ë¨¸ì‹ ëŸ¬ë‹ì—ì„œ ê³¼ì í•©ì„ ë°©ì§€í•˜ëŠ” ë°©ë²•ì€?",
        "Dockerfile ìµœì í™” íŒì„ ì•Œë ¤ì¤˜.",
        "Git ë¸Œëœì¹˜ ì „ëµì„ ì„¤ëª…í•´ì¤˜."
    ]
    results = []
    for q in test_questions:
        try:
            response = requests.post(
                "http://localhost:11434/api/generate",
                json={
                    "model": f"{MODEL_BASE_NAME.lower()}-finetune",
                    "prompt": q,
                    "stream": False,
                    "options": {"num_predict": 128}
                },
                timeout=60
            )
            if response.status_code == 200:
                answer = response.json().get('response', '')
                print(f"Q: {q}\nA: {answer[:200]}\n{'-'*40}")
                results.append({"question": q, "answer": answer})
            else:
                print(f"âŒ ì‘ë‹µ ì‹¤íŒ¨: {response.status_code}")
                results.append({"question": q, "error": f"HTTP {response.status_code}"})
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜: {e}")
            results.append({"question": q, "error": str(e)})
    # ê²°ê³¼ ì €ì¥
    logs_dir = Path("logs")
    logs_dir.mkdir(exist_ok=True)
    with open(logs_dir / "ollama_inference_test.json", "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    print(f"   âœ… ê²°ê³¼ ì €ì¥: {logs_dir / 'ollama_inference_test.json'}")
    return results

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ 6ë‹¨ê³„: ì„œë¹„ìŠ¤ ìµœì í™” ë° ë°°í¬ ì‹œì‘")
    print("=" * 50)
    
    try:
        # 1. Ollama ì„œë¹„ìŠ¤ í™•ì¸/ì‹œì‘
        if not check_ollama_service():
            if not start_ollama_service():
                print("âŒ Ollama ì„œë¹„ìŠ¤ ì‹œì‘ ì‹¤íŒ¨")
                return False
        # 1-1. Ollamaì— ëª¨ë¸ ë“±ë¡
        if not register_model_to_ollama():
            print("âŒ Ollama ëª¨ë¸ ë“±ë¡ ì‹¤íŒ¨")
            return False
        # 1-2. Ollama ì§ˆë¬¸-ì‘ë‹µ í…ŒìŠ¤íŠ¸
        test_ollama_inference()
        
        # 2. ì„±ëŠ¥ ìµœì í™”
        optimize_ollama_performance()
        
        # 3. ëª¨ë¸ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬
        benchmark_results = benchmark_model_performance()
        
        # 4. ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸
        stress_results = stress_test_concurrent_requests()
        
        # 5. API ë˜í¼ ìƒì„±
        api_file = create_api_wrapper()
        
        # 6. ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ ìƒì„±
        dashboard_file = create_monitoring_dashboard()
        
        # 7. ë°°í¬ ë³´ê³ ì„œ ì €ì¥
        save_deployment_report(benchmark_results, stress_results)
        
        print("=" * 50)
        print("âœ… 6ë‹¨ê³„ ì™„ë£Œ!")
        print("ğŸ‰ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
        
        print("\nğŸ“‹ ìµœì¢… ê²°ê³¼:")
        if benchmark_results:
            avg_speed = sum(r['tokens_per_sec'] for r in benchmark_results) / len(benchmark_results)
            print(f"   í‰ê·  ìƒì„± ì†ë„: {avg_speed:.1f} í† í°/ì´ˆ")
        
        if stress_results:
            valid_concurrent = [r['concurrent_requests'] for r in stress_results if r['success_rate'] > 80]
            if valid_concurrent:
                max_concurrent = max(valid_concurrent)
                print(f"   ê¶Œì¥ ë™ì‹œ ì‚¬ìš©ì: {max_concurrent}ëª…")
            else:
                print("   ê¶Œì¥ ë™ì‹œ ì‚¬ìš©ì: (OllamaëŠ” ë™ì‹œ ìš”ì²­ì„ ê³µì‹ ì§€ì›í•˜ì§€ ì•ŠìŒ)")
        
        print("\nğŸš€ ì„œë¹„ìŠ¤ ì‹œì‘ ë°©ë²•:")
        print(f"   1. Ollama: ollama run {OLLAMA_CONFIG['model_name']}")
        print(f"   2. API ì„œë²„: python3 {api_file}")
        print(f"   3. ëª¨ë‹ˆí„°ë§: ë¸Œë¼ìš°ì €ì—ì„œ {dashboard_file} ì—´ê¸°")
        
        print("\nğŸ“Š API ì—”ë“œí¬ì¸íŠ¸:")
        print("   - GET  http://localhost:8000/health")
        print("   - POST http://localhost:8000/chat")
        print("   - Ollama http://localhost:11434/api/generate")
        
        return True
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1) 