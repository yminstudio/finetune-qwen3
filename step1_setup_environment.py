#!/usr/bin/env python3
"""
ğŸ¯ 1ë‹¨ê³„: ëª¨ë¸ ì¤€ë¹„ ë° í™˜ê²½ ì„¤ì • (30ë¶„)

ëª©ì :
- í‘œì¤€ transformers í™˜ê²½ êµ¬ì¶•  
- Qwen3-8B ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ìºì‹œ
- GPU ë©”ëª¨ë¦¬ ë° í™˜ê²½ ìµœì í™”
"""

import os
import sys
import time
import torch
import psutil
import GPUtil
from pathlib import Path
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
import gc
from config import PROJECT_ROOT

# ì„¤ì • íŒŒì¼ import
from config import *

LOG_DIR = os.path.join(os.path.dirname(__file__), 'logs')
LOG_FILE = os.path.join(LOG_DIR, 'step1_setup_environment.log')
os.makedirs(LOG_DIR, exist_ok=True)
def log_print(*args, **kwargs):
    print(*args, **kwargs)
    with open(LOG_FILE, 'a', encoding='utf-8') as f:
        print(*args, **kwargs, file=f)

def setup_cuda_environment():
    """CUDA í™˜ê²½ ìµœì í™”"""
    log_print("ğŸ”§ CUDA í™˜ê²½ ì„¤ì • ì¤‘...")
    
    # GPU ë©”ëª¨ë¦¬ í• ë‹¹ ì „ëµ ì„¤ì •
    os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
    
    # CUDA ê°€ì‹œì„± ì„¤ì • (í•„ìš”ì‹œ)
    if "CUDA_VISIBLE_DEVICES" not in os.environ:
        os.environ["CUDA_VISIBLE_DEVICES"] = "0"
    
    # GPU ì •ë³´ ì¶œë ¥
    if torch.cuda.is_available():
        gpu_count = torch.cuda.device_count()
        log_print(f"âœ… CUDA ì‚¬ìš© ê°€ëŠ¥ - GPU ê°œìˆ˜: {gpu_count}")
        
        for i in range(gpu_count):
            gpu_props = torch.cuda.get_device_properties(i)
            memory_gb = gpu_props.total_memory / (1024**3)
            log_print(f"   GPU {i}: {gpu_props.name} ({memory_gb:.1f}GB)")
        
        # í˜„ì¬ GPU ë©”ëª¨ë¦¬ ìƒíƒœ
        gpus = GPUtil.getGPUs()
        for gpu in gpus:
            memory_percent = gpu.memoryUtil * 100
            log_print(f"   GPU {gpu.id}: ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  {memory_percent:.1f}% ({gpu.memoryUsed}MB/{gpu.memoryTotal}MB)")
    else:
        log_print("âŒ CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
        return False
    
    return True

def check_system_resources():
    """ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ í™•ì¸"""
    log_print("ğŸ’¾ ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ í™•ì¸ ì¤‘...")
    
    # RAM í™•ì¸
    memory = psutil.virtual_memory()
    swap = psutil.swap_memory()
    
    log_print(f"   RAM: {memory.used/(1024**3):.1f}GB / {memory.total/(1024**3):.1f}GB ({memory.percent:.1f}%)")
    log_print(f"   Swap: {swap.used/(1024**3):.1f}GB / {swap.total/(1024**3):.1f}GB ({swap.percent:.1f}%)")
    
    # ë””ìŠ¤í¬ ê³µê°„ í™•ì¸
    disk = psutil.disk_usage('/')
    log_print(f"   ë””ìŠ¤í¬: {disk.used/(1024**3):.1f}GB / {disk.total/(1024**3):.1f}GB ({(disk.used/disk.total)*100:.1f}%)")
    
    # ìµœì†Œ ìš”êµ¬ì‚¬í•­ ì²´í¬ (8B ëª¨ë¸ ê¸°ì¤€)
    warnings = []
    if memory.total < 16 * (1024**3):  # 16GB
        warnings.append(f"âš ï¸ RAMì´ ë¶€ì¡±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê¶Œì¥: 24GB+, ìµœì†Œ: 16GB, í˜„ì¬: {memory.total/(1024**3):.1f}GB")
    
    if swap.total < 16 * (1024**3):  # 16GB
        warnings.append(f"âš ï¸ Swap ë©”ëª¨ë¦¬ê°€ ë¶€ì¡±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê¶Œì¥: 16GB+, í˜„ì¬: {swap.total/(1024**3):.1f}GB")
    
    if disk.free < 40 * (1024**3):  # 40GB
        warnings.append(f"âš ï¸ ë””ìŠ¤í¬ ê³µê°„ì´ ë¶€ì¡±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìµœì†Œ: 40GB, í˜„ì¬ ì—¬ìœ : {disk.free/(1024**3):.1f}GB")
    
    for warning in warnings:
        log_print(warning)
    
    return len(warnings) == 0

def download_and_cache_model():
    """ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ìºì‹œ"""
    log_print(f"ğŸ”„ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹œì‘: {MODEL_NAME}")
    start_time = time.time()
    
    try:
        # BitsAndBytesConfig ì„¤ì •
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.bfloat16,
            bnb_4bit_use_double_quant=False,
        )
        
        log_print("   í† í¬ë‚˜ì´ì € ë¡œë“œ ì¤‘...")
        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
        
        log_print("   4-bit ì–‘ìí™” ëª¨ë¸ ë¡œë“œ ì¤‘...")
        model = AutoModelForCausalLM.from_pretrained(
            MODEL_NAME,
            quantization_config=bnb_config,
            torch_dtype=torch.bfloat16,
            device_map="auto",
            trust_remote_code=True
        )
        
        # ê¸°ë³¸ í…ŒìŠ¤íŠ¸
        log_print("   ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì¤‘...")
        messages = [
            {"role": "user", "content": "ì•ˆë…•í•˜ì„¸ìš”! ê°„ë‹¨í•œ ì¸ì‚¬ë§ì„ í•´ì£¼ì„¸ìš”."}
        ]
        
        text = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True,
            enable_thinking=False  # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ thinking ëª¨ë“œ ë¹„í™œì„±í™”
        )
        
        inputs = tokenizer([text], return_tensors="pt").to(model.device)
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=50,
                temperature=0.7,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
        
        output_ids = outputs[0][len(inputs.input_ids[0]):].tolist()
        response = tokenizer.decode(output_ids, skip_special_tokens=True)
        log_print(f"   í…ŒìŠ¤íŠ¸ ì¶œë ¥: {response[:100]}...")
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        del model, tokenizer, inputs, outputs
        torch.cuda.empty_cache()
        gc.collect()
        
        elapsed_time = time.time() - start_time
        log_print(f"âœ… ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ ({elapsed_time:.1f}ì´ˆ)")
        
        # ìºì‹œ ìœ„ì¹˜ ì •ë³´
        cache_dir = Path.home() / ".cache" / "huggingface" / "hub" / f"models--Qwen--{MODEL_BASE_NAME}"
        if cache_dir.exists():
            cache_size = sum(f.stat().st_size for f in cache_dir.rglob('*') if f.is_file())
            log_print(f"   ìºì‹œ ìœ„ì¹˜: {cache_dir}")
            log_print(f"   ìºì‹œ í¬ê¸°: {cache_size/(1024**3):.1f}GB")
        
        return True
        
    except Exception as e:
        log_print(f"âŒ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return False

def verify_dependencies():
    """ì˜ì¡´ì„± íŒ¨í‚¤ì§€ í™•ì¸"""
    log_print("ğŸ“¦ ì˜ì¡´ì„± íŒ¨í‚¤ì§€ í™•ì¸ ì¤‘...")
    
    required_packages = [
        ("torch", "2.0.0"),
        ("transformers", "4.51.0"), 
        ("peft", "0.12.0"),
        ("bitsandbytes", "0.43.0"),
        ("accelerate", "0.30.0")
    ]
    
    missing_packages = []
    
    for package_name, min_version in required_packages:
        try:
            pkg = __import__(package_name)
            version = getattr(pkg, '__version__', 'unknown')
            log_print(f"   âœ… {package_name}: {version}")
        except ImportError:
            log_print(f"   âŒ {package_name}: ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ")
            missing_packages.append(package_name)
    
    if missing_packages:
        log_print(f"âš ï¸ ëˆ„ë½ëœ íŒ¨í‚¤ì§€: {', '.join(missing_packages)}")
        log_print("   ë‹¤ìŒ ëª…ë ¹ìœ¼ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”: pip install -r requirements.txt")
        return False
    
    return True

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    log_print("ğŸš€ 1ë‹¨ê³„: ëª¨ë¸ ì¤€ë¹„ ë° í™˜ê²½ ì„¤ì • ì‹œì‘")
    log_print("=" * 50)
    
    # 1. ì˜ì¡´ì„± í™•ì¸
    if not verify_dependencies():
        log_print("âŒ ì˜ì¡´ì„± íŒ¨í‚¤ì§€ ì„¤ì¹˜ í›„ ë‹¤ì‹œ ì‹¤í–‰í•˜ì„¸ìš”.")
        return False
    
    # 2. CUDA í™˜ê²½ ì„¤ì •
    if not setup_cuda_environment():
        log_print("âŒ CUDA í™˜ê²½ ì„¤ì • ì‹¤íŒ¨")
        return False
    
    # 3. ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ í™•ì¸
    check_system_resources()
    
    # 4. ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° í…ŒìŠ¤íŠ¸
    if not download_and_cache_model():
        log_print("âŒ ëª¨ë¸ ì¤€ë¹„ ì‹¤íŒ¨")
        return False
    
    log_print("=" * 50)
    log_print("âœ… 1ë‹¨ê³„ ì™„ë£Œ!")
    log_print("ğŸ“‹ ë‹¤ìŒ ë‹¨ê³„ ì•ˆë‚´:")
    log_print("   - 2ë‹¨ê³„ ì‹¤í–‰: python step2_train_qlora.py")
    log_print(f"   - í•™ìŠµ ë°ì´í„°ë¥¼ {PROJECT_ROOT / 'data'} í´ë”ì— ì¤€ë¹„í•´ì£¼ì„¸ìš”")
    
    return True

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1) 