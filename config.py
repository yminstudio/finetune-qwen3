"""
QLora íŒŒì¸íŠœë‹ â†’ ì„œë¹„ìŠ¤ ë°°í¬ í”„ë¡œì íŠ¸ ì„¤ì •
"""
import os
from pathlib import Path

# ëª¨ë¸ ì„¤ì •
MODEL_BASE_NAME = "Qwen3-4B"
MODEL_NAME = f"Qwen/{MODEL_BASE_NAME}"

# ëª¨ë¸ë³„ ë©”ëª¨ë¦¬ ìš”êµ¬ì‚¬í•­ (GB ë‹¨ìœ„)
MODEL_MEMORY_REQUIREMENTS = {
    "Qwen3-4B": {
        "training_ram": 16,      # í•™ìŠµ ì‹œ ìµœì†Œ RAM
        "merge_ram": 12,         # ë³‘í•© ì‹œ ìµœì†Œ RAM  
        "inference_ram": 8,      # ì¶”ë¡  ì‹œ ìµœì†Œ RAM
        "disk_space": 15,        # í•„ìš” ë””ìŠ¤í¬ ê³µê°„
        "gpu_vram": 12          # í•„ìš” GPU VRAM
    },
    "Qwen3-8B": {
        "training_ram": 32,
        "merge_ram": 24,
        "inference_ram": 16,
        "disk_space": 30,
        "gpu_vram": 24
    },
    "Qwen3-14B": {
        "training_ram": 48,
        "merge_ram": 36,
        "inference_ram": 24,
        "disk_space": 50,
        "gpu_vram": 36
    }
}

def get_memory_requirements():
    """í˜„ì¬ ëª¨ë¸ì˜ ë©”ëª¨ë¦¬ ìš”êµ¬ì‚¬í•­ ë°˜í™˜"""
    return MODEL_MEMORY_REQUIREMENTS.get(MODEL_BASE_NAME, MODEL_MEMORY_REQUIREMENTS["Qwen3-4B"])

# í”„ë¡œì íŠ¸ ê¸°ë³¸ ê²½ë¡œ
PROJECT_ROOT = Path(__file__).parent
CACHE_DIR = PROJECT_ROOT / "cache"
OUTPUT_DIR = PROJECT_ROOT / f"outputs/{MODEL_BASE_NAME}/"
MODELS_DIR = PROJECT_ROOT / "models"
LOGS_DIR = PROJECT_ROOT / "logs"

# í•„ìš”í•œ ë””ë ‰í† ë¦¬ ìƒì„±
for dir_path in [CACHE_DIR, OUTPUT_DIR, MODELS_DIR, LOGS_DIR]:
    dir_path.mkdir(exist_ok=True)

# QLora ì„¤ì •
QLORA_CONFIG = {
    "r": 16,                    # LoRA rank
    "lora_alpha": 16,          # LoRA alpha parameter
    "target_modules": [
        "q_proj", "k_proj", "v_proj", "o_proj",
        "gate_proj", "up_proj", "down_proj"
    ],
    "lora_dropout": 0.1,       # LoRA dropout
    "bias": "none",
    "task_type": "CAUSAL_LM"
}

# í•™ìŠµ ì„¤ì •
TRAINING_CONFIG = {
    "output_dir": str(OUTPUT_DIR / "qlora_checkpoints"),
    "num_train_epochs": 1,
    "per_device_train_batch_size": 1,
    "gradient_accumulation_steps": 4,
    "optim": "adamw_8bit",
    "save_steps": 50,
    "logging_steps": 10,
    "learning_rate": 2e-4,
    "weight_decay": 0.001,
    "fp16": False,
    "bf16": True,
    "max_grad_norm": 0.3,
    "max_steps": 100,
    "warmup_ratio": 0.03,
    "group_by_length": True,
    "lr_scheduler_type": "linear",
    "report_to": [],  # wandb ë¹„í™œì„±í™” (ë¹ˆ ë¦¬ìŠ¤íŠ¸)
    "eval_strategy": "no"
}

# 4-bit ì–‘ìí™” ì„¤ì •
BNB_CONFIG = {
    "load_in_4bit": True,
    "bnb_4bit_quant_type": "nf4",
    "bnb_4bit_compute_dtype": "bfloat16",
    "bnb_4bit_use_double_quant": False,
}

# ë°ì´í„° ì„¤ì •
MAX_SEQ_LENGTH = 2048
CUTOFF_LEN = 512

# GPU ì„¤ì •
GPU_CONFIG = {
    "torch_dtype": "bfloat16",
    "attn_implementation": "flash_attention_2",
    "device_map": "auto"
}

# GGUF ë³€í™˜ ì„¤ì •
GGUF_CONFIG = {
    "quantization_type": "q4_k_m",  # ê¶Œì¥ ì˜µì…˜: ì†ë„ì™€ í’ˆì§ˆ ê· í˜•
    "context_length": 4096,
    "output_filename": f"{MODEL_BASE_NAME.lower()}-finetune-q4_k_m.gguf"
}

# Ollama ì„¤ì •
OLLAMA_CONFIG = {
    "model_name": f"{MODEL_BASE_NAME.lower()}-finetune",  # ê°„ë‹¨í•œ í˜•ì‹ìœ¼ë¡œ ë³€ê²½
    "temperature": 0.7,
    "top_p": 0.9,
    "repeat_penalty": 1.1,
    "stop_tokens": ["<|im_end|>", "</s>"],
    "num_ctx": 4096,
    "num_keep": 24
}

# API ì„œë²„ ì„¤ì •
API_CONFIG = {
    "host": "0.0.0.0",
    "port": 8000,
    "workers": 1,
    "timeout": 300
}

# ë¡œê¹… ì„¤ì •
LOGGING_CONFIG = {
    "level": "INFO",
    "format": "%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    "filename": str(LOGS_DIR / "pipeline.log")
}

# ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì„¤ì •
MONITORING_CONFIG = {
    "log_memory_usage": True,
    "log_gpu_usage": True,
    "benchmark_inference": True,
    "save_metrics": True
}

LLAMA_CPP_DIR = Path("/root/llama.cpp")

print(f"âœ… ì„¤ì • ë¡œë“œ ì™„ë£Œ")
print(f"ğŸ“ í”„ë¡œì íŠ¸ ë£¨íŠ¸: {PROJECT_ROOT}")
print(f"ğŸ¤– ëª¨ë¸: {MODEL_NAME}")
print(f"ğŸ’¾ ì¶œë ¥ ë””ë ‰í† ë¦¬: {OUTPUT_DIR}") 