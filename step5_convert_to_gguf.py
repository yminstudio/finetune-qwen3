#!/usr/bin/env python3
"""
âš¡ 5ë‹¨ê³„: GGUF ë³€í™˜ ë° Ollama ë“±ë¡ (1-2ì‹œê°„)

ëª©ì :
- 16-bit ëª¨ë¸ì„ ì¶”ë¡  ìµœì í™”ëœ GGUF í¬ë§·ìœ¼ë¡œ ë³€í™˜
- Ollama ì—”ì§„ì—ì„œ ê³ ì† ì¶”ë¡  ê°€ëŠ¥í•˜ë„ë¡ ì¤€ë¹„
- ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ê³¼ ì†ë„ ìµœì í™”
"""

import os
import sys
import time
import json
import subprocess
import shutil
from pathlib import Path

# ì„¤ì • íŒŒì¼ import
from config import *
from config import LLAMA_CPP_DIR
from config import MODELS_DIR
from config import PROJECT_ROOT

LOG_DIR = os.path.join(os.path.dirname(__file__), 'logs')
LOG_FILE = os.path.join(LOG_DIR, 'step5_convert_to_gguf.log')
os.makedirs(LOG_DIR, exist_ok=True)
def log_print(*args, **kwargs):
    print(*args, **kwargs)
    with open(LOG_FILE, 'a', encoding='utf-8') as f:
        print(*args, **kwargs, file=f)

def check_prerequisites():
    """í•„ìˆ˜ ë„êµ¬ í™•ì¸"""
    log_print("ğŸ” í•„ìˆ˜ ë„êµ¬ í™•ì¸ ì¤‘...")
    
    prerequisites = {
        "git": "git --version",
        "python3": "python3 --version",
        "cmake": "cmake --version",
        "make": "make --version"
    }
    
    missing_tools = []
    
    for tool, check_cmd in prerequisites.items():
        try:
            result = subprocess.run(check_cmd.split(), 
                                  capture_output=True, text=True, check=True)
            version_info = result.stdout.strip().split('\n')[0]  # ì²« ë²ˆì§¸ ì¤„ë§Œ
            log_print(f"   âœ… {tool}: {version_info.split()[-1] if version_info else 'OK'}")
        except (subprocess.CalledProcessError, FileNotFoundError):
            log_print(f"   âŒ {tool}: ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ")
            missing_tools.append(tool)
    
    if missing_tools:
        log_print(f"\nâš ï¸ ëˆ„ë½ëœ ë„êµ¬ë“¤ì„ ì„¤ì¹˜í•˜ì„¸ìš”:")
        if "cmake" in missing_tools:
            log_print("   sudo apt-get install cmake")
        if "make" in missing_tools:
            log_print("   sudo apt-get install build-essential")
        if "git" in missing_tools:
            log_print("   sudo apt-get install git")
        return False
    
    return True

def setup_llama_cpp():
    """llama.cpp í™˜ê²½ êµ¬ì¶•"""
    log_print("ğŸ› ï¸ llama.cpp í™˜ê²½ êµ¬ì¶• ì¤‘...")
    
    llama_cpp_dir = LLAMA_CPP_DIR
    
    if llama_cpp_dir.exists():
        log_print("   llama.cpp ë””ë ‰í† ë¦¬ê°€ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤.")
        # ë¹Œë“œ íŒŒì¼ë“¤ì´ ìˆëŠ”ì§€ í™•ì¸
        convert_script = llama_cpp_dir / "convert_hf_to_gguf.py"
        quantize_binary = llama_cpp_dir / "llama-quantize"
        if convert_script.exists() and quantize_binary.exists():
            log_print("   âœ… ì´ë¯¸ ë¹Œë“œë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
            return llama_cpp_dir
        else:
            log_print("   ë¹Œë“œ íŒŒì¼ì´ ì—†ì–´ì„œ ë‹¤ì‹œ ë¹Œë“œí•©ë‹ˆë‹¤.")
    if not llama_cpp_dir.exists():
        log_print("   llama.cpp ì €ì¥ì†Œ í´ë¡  ì¤‘...")
        try:
            subprocess.run([
                "git", "clone", 
                "https://github.com/ggerganov/llama.cpp.git",
                str(llama_cpp_dir)
            ], check=True, cwd=PROJECT_ROOT)
            log_print("   âœ… í´ë¡  ì™„ë£Œ")
        except subprocess.CalledProcessError as e:
            log_print(f"   âŒ í´ë¡  ì‹¤íŒ¨: {e}")
            return None
    # CMakeë¥¼ ì‚¬ìš©í•œ ë¹Œë“œ
    build_dir = llama_cpp_dir / "build"
    build_dir.mkdir(exist_ok=True)
    log_print("   CMake ì„¤ì • ì¤‘...")
    try:
        # CUDA ì§€ì› CMake ì„¤ì •
        cmake_cmd = [
            "cmake", "..", 
            "-DGGML_CUDA=ON",
            "-DCMAKE_BUILD_TYPE=Release"
        ]
        result = subprocess.run(cmake_cmd, check=True, cwd=build_dir, 
                              capture_output=True, text=True)
        log_print("   âœ… CMake ì„¤ì • ì™„ë£Œ (CUDA ì§€ì›)")
    except subprocess.CalledProcessError as e:
        log_print(f"   âš ï¸ CUDA CMake ì‹¤íŒ¨, CPU ë²„ì „ìœ¼ë¡œ ì‹œë„: {e}")
        # CUDA ì—†ì´ CPU ë²„ì „ìœ¼ë¡œ ëŒ€ì²´
        try:
            cmake_cmd = [
                "cmake", "..", 
                "-DCMAKE_BUILD_TYPE=Release"
            ]
            subprocess.run(cmake_cmd, check=True, cwd=build_dir,
                          capture_output=True, text=True)
            log_print("   âœ… CMake ì„¤ì • ì™„ë£Œ (CPU ë²„ì „)")
        except subprocess.CalledProcessError as e2:
            log_print(f"   âŒ CMake ì„¤ì • ì‹¤íŒ¨: {e2}")
            return None
    log_print("   ë¹Œë“œ ì¤‘...")
    try:
        # makeë¥¼ ì‚¬ìš©í•œ ë¹Œë“œ
        subprocess.run([
            "make", "-j", str(os.cpu_count() or 4)
        ], check=True, cwd=build_dir)
        log_print("   âœ… ë¹Œë“œ ì™„ë£Œ")
        # ë¹Œë“œëœ ë°”ì´ë„ˆë¦¬ë¥¼ ìƒìœ„ ë””ë ‰í† ë¦¬ë¡œ ë³µì‚¬
        quantize_src = build_dir / "bin" / "llama-quantize"
        quantize_dst = llama_cpp_dir / "llama-quantize"
        if quantize_src.exists():
            shutil.copy2(quantize_src, quantize_dst)
            log_print("   âœ… llama-quantize ë³µì‚¬ ì™„ë£Œ")
        else:
            # ë‹¤ë¥¸ ìœ„ì¹˜ í™•ì¸
            alt_paths = [
                build_dir / "llama-quantize",
                build_dir / "quantize"
            ]
            for alt_path in alt_paths:
                if alt_path.exists():
                    shutil.copy2(alt_path, quantize_dst)
                    log_print(f"   âœ… llama-quantize ë³µì‚¬ ì™„ë£Œ ({alt_path})")
                    break
            else:
                log_print("   âš ï¸ llama-quantize ë°”ì´ë„ˆë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    except subprocess.CalledProcessError as e:
        log_print(f"   âŒ ë¹Œë“œ ì‹¤íŒ¨: {e}")
        return None
    # ë³€í™˜ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ê¶Œí•œ í™•ì¸
    convert_script = llama_cpp_dir / "convert_hf_to_gguf.py"
    if not convert_script.exists():
        log_print(f"   âŒ ë³€í™˜ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {convert_script}")
        return None
    return llama_cpp_dir

def convert_hf_to_gguf(merged_model_dir, llama_cpp_dir):
    """HuggingFace ëª¨ë¸ì„ GGUFë¡œ ë³€í™˜"""
    log_print("ğŸ”„ HuggingFace â†’ GGUF ë³€í™˜ ì¤‘...")
    
    if not merged_model_dir.exists():
        log_print("âŒ ë³‘í•©ëœ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        log_print("   ë¨¼ì € 4ë‹¨ê³„ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”: python3 step4_merge_adapters.py")
        return None
    
    # ì¶œë ¥ íŒŒì¼ëª… ì„¤ì •
    gguf_output_dir = OUTPUT_DIR / "gguf_models"
    gguf_output_dir.mkdir(exist_ok=True)
    
    gguf_filename = f"{MODEL_BASE_NAME}-korean-f16.gguf"
    gguf_output_path = gguf_output_dir / gguf_filename
    
    # ë³€í™˜ ì‹¤í–‰
    convert_script = llama_cpp_dir / "convert_hf_to_gguf.py"
    
    log_print(f"   ì…ë ¥: {merged_model_dir}")
    log_print(f"   ì¶œë ¥: {gguf_output_path}")
    
    start_time = time.time()
    
    try:
        cmd = [
            "python3", str(convert_script),
            str(merged_model_dir),
            "--outfile", str(gguf_output_path),
            "--outtype", "f16"  # 16-bit ë¶€ë™ì†Œìˆ˜ì 
        ]
        
        log_print(f"   ì‹¤í–‰ ëª…ë ¹: {' '.join(cmd)}")
        
        result = subprocess.run(cmd, check=True, capture_output=True, text=True)
        
        conversion_time = time.time() - start_time
        log_print(f"   âœ… ë³€í™˜ ì™„ë£Œ ({conversion_time:.1f}ì´ˆ)")
        
        # íŒŒì¼ í¬ê¸° í™•ì¸
        if gguf_output_path.exists():
            size_gb = gguf_output_path.stat().st_size / (1024**3)
            log_print(f"   GGUF íŒŒì¼ í¬ê¸°: {size_gb:.1f}GB")
        
        return gguf_output_path
        
    except subprocess.CalledProcessError as e:
        log_print(f"   âŒ ë³€í™˜ ì‹¤íŒ¨: {e}")
        if e.stderr:
            log_print(f"   ì˜¤ë¥˜ ë©”ì‹œì§€: {e.stderr}")
        return None

def quantize_gguf_model(gguf_path, llama_cpp_dir):
    """GGUF ëª¨ë¸ ì–‘ìí™”"""
    log_print("ğŸ—œï¸ GGUF ëª¨ë¸ ì–‘ìí™” ì¤‘...")
    
    # ì–‘ìí™”ëœ ëª¨ë¸ ì¶œë ¥ ê²½ë¡œ
    quantized_filename = GGUF_CONFIG["output_filename"]
    quantized_path = gguf_path.parent / quantized_filename
    
    # quantize ë°”ì´ë„ˆë¦¬ í™•ì¸
    quantize_binary = llama_cpp_dir / "llama-quantize"
    if not quantize_binary.exists():
        quantize_binary = llama_cpp_dir / "quantize"  # ì´ì „ ë²„ì „ í˜¸í™˜ì„±
    
    if not quantize_binary.exists():
        log_print("   âŒ quantize ë°”ì´ë„ˆë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None
    
    quant_type = GGUF_CONFIG["quantization_type"].upper()
    log_print(f"   ì–‘ìí™” íƒ€ì…: {quant_type}")
    log_print(f"   ì…ë ¥: {gguf_path}")
    log_print(f"   ì¶œë ¥: {quantized_path}")
    
    start_time = time.time()
    
    try:
        cmd = [
            str(quantize_binary),
            str(gguf_path),
            str(quantized_path),
            quant_type
        ]
        
        log_print(f"   ì‹¤í–‰ ëª…ë ¹: {' '.join(cmd)}")
        
        result = subprocess.run(cmd, check=True, capture_output=True, text=True)
        
        quantization_time = time.time() - start_time
        log_print(f"   âœ… ì–‘ìí™” ì™„ë£Œ ({quantization_time:.1f}ì´ˆ)")
        
        # ì••ì¶•ë¥  ê³„ì‚°
        if quantized_path.exists():
            original_size = gguf_path.stat().st_size
            quantized_size = quantized_path.stat().st_size
            compression_ratio = quantized_size / original_size
            
            log_print(f"   ì›ë³¸ í¬ê¸°: {original_size/(1024**3):.1f}GB")
            log_print(f"   ì–‘ìí™” í¬ê¸°: {quantized_size/(1024**3):.1f}GB")
            log_print(f"   ì••ì¶•ë¥ : {compression_ratio:.2f} ({(1-compression_ratio)*100:.1f}% ì ˆì•½)")
        
        return quantized_path
        
    except subprocess.CalledProcessError as e:
        log_print(f"   âŒ ì–‘ìí™” ì‹¤íŒ¨: {e}")
        if e.stderr:
            log_print(f"   ì˜¤ë¥˜ ë©”ì‹œì§€: {e.stderr}")
        return None

def check_ollama_installation():
    """Ollama ì„¤ì¹˜ í™•ì¸"""
    log_print("ğŸ” Ollama ì„¤ì¹˜ í™•ì¸ ì¤‘...")
    
    try:
        result = subprocess.run(["ollama", "--version"], 
                              capture_output=True, text=True, check=True)
        version = result.stdout.strip()
        log_print(f"   âœ… Ollama ì„¤ì¹˜ë¨: {version}")
        return True
    except (subprocess.CalledProcessError, FileNotFoundError):
        log_print("   âŒ Ollamaê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        log_print("   ì„¤ì¹˜ ë°©ë²•: curl -fsSL https://ollama.com/install.sh | sh")
        return False

def create_ollama_modelfile(quantized_gguf_path):
    """Ollama Modelfile ìƒì„±"""
    log_print("ğŸ“ Ollama Modelfile ìƒì„± ì¤‘...")
    
    modelfile_content = f"""FROM {quantized_gguf_path.name}

TEMPLATE \"\"\"<|im_start|>system
{{{{ .System }}}}<|im_end|>
<|im_start|>user
{{{{ .Prompt }}}}<|im_end|>
<|im_start|>assistant
\"\"\"

PARAMETER temperature {OLLAMA_CONFIG['temperature']}
PARAMETER top_p {OLLAMA_CONFIG['top_p']}
PARAMETER repeat_penalty {OLLAMA_CONFIG['repeat_penalty']}
PARAMETER num_ctx {OLLAMA_CONFIG['num_ctx']}
PARAMETER num_keep {OLLAMA_CONFIG['num_keep']}

SYSTEM \"\"\"ë‹¹ì‹ ì€ í•œêµ­ì–´ ê¸°ìˆ  ì „ë¬¸ê°€ AIì…ë‹ˆë‹¤. íŒŒì´ì¬, ë¨¸ì‹ ëŸ¬ë‹, ì›¹ê°œë°œ, Docker, Git ë“±ì˜ ê¸°ìˆ ì  ì§ˆë¬¸ì— ì •í™•í•˜ê³  ì‹¤ìš©ì ì¸ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.\"\"\"
"""
    
    modelfile_path = quantized_gguf_path.parent / f"{MODEL_BASE_NAME.lower()}-finetune-modelfile"
    
    with open(modelfile_path, 'w', encoding='utf-8') as f:
        f.write(modelfile_content)
    
    log_print(f"   âœ… Modelfile ìƒì„±: {modelfile_path}")
    return modelfile_path

def register_ollama_model(modelfile_path):
    """Ollamaì— ëª¨ë¸ ë“±ë¡"""
    log_print("ğŸ“‹ Ollamaì— ëª¨ë¸ ë“±ë¡ ì¤‘...")
    
    model_name = OLLAMA_CONFIG['model_name']
    
    try:
        cmd = ["ollama", "create", model_name, "-f", str(modelfile_path)]
        log_print(f"   ì‹¤í–‰ ëª…ë ¹: {' '.join(cmd)}")
        
        result = subprocess.run(cmd, check=True, capture_output=True, text=True)
        
        log_print(f"   âœ… ëª¨ë¸ ë“±ë¡ ì™„ë£Œ: {model_name}")
        return model_name
        
    except subprocess.CalledProcessError as e:
        log_print(f"   âŒ ëª¨ë¸ ë“±ë¡ ì‹¤íŒ¨: {e}")
        if e.stderr:
            log_print(f"   ì˜¤ë¥˜ ë©”ì‹œì§€: {e.stderr}")
        return None

def test_ollama_model(model_name):
    """ë“±ë¡ëœ Ollama ëª¨ë¸ í…ŒìŠ¤íŠ¸"""
    log_print("ğŸ§ª Ollama ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì¤‘...")
    
    test_prompts = [
        "ì•ˆë…•í•˜ì„¸ìš”! ê°„ë‹¨í•œ ìê¸°ì†Œê°œë¥¼ í•´ì£¼ì„¸ìš”.",
        "íŒŒì´ì¬ ë¦¬ìŠ¤íŠ¸ì™€ íŠœí”Œì˜ ì°¨ì´ì ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
        "ë¨¸ì‹ ëŸ¬ë‹ì—ì„œ ê³¼ì í•©ì´ë€ ë¬´ì—‡ì¸ê°€ìš”?"
    ]
    
    for i, prompt in enumerate(test_prompts):
        log_print(f"\n   í…ŒìŠ¤íŠ¸ {i+1}/3: {prompt[:30]}...")
        
        try:
            start_time = time.time()
            
            cmd = ["ollama", "run", model_name, prompt]
            result = subprocess.run(cmd, capture_output=True, text=True, 
                                  timeout=60, check=True)
            
            response_time = time.time() - start_time
            response = result.stdout.strip()
            
            log_print(f"   ì‘ë‹µ ì‹œê°„: {response_time:.1f}ì´ˆ")
            log_print(f"   ì‘ë‹µ: {response[:150]}...")
            
        except subprocess.TimeoutExpired:
            log_print("   âš ï¸ ì‘ë‹µ ì‹œê°„ ì´ˆê³¼")
        except subprocess.CalledProcessError as e:
            log_print(f"   âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            return False
    
    log_print("   âœ… ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
    return True

def cleanup_temp_files():
    """ì„ì‹œ íŒŒì¼ ì •ë¦¬"""
    log_print("ğŸ§¹ ì„ì‹œ íŒŒì¼ ì •ë¦¬ ì¤‘...")
    
    # F16 GGUF íŒŒì¼ ì‚­ì œ (ì–‘ìí™”ëœ ë²„ì „ë§Œ ìœ ì§€)
    gguf_dir = OUTPUT_DIR / "gguf_models"
    if gguf_dir.exists():
        for f16_file in gguf_dir.glob("*-f16.gguf"):
            try:
                f16_file.unlink()
                log_print(f"   ì‚­ì œ: {f16_file.name}")
            except OSError:
                pass
    
    log_print("   ì„ì‹œ íŒŒì¼ ì •ë¦¬ ì™„ë£Œ")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    log_print("âš¡ 5ë‹¨ê³„: GGUF ë³€í™˜ ë° Ollama ë“±ë¡ ì‹œì‘")
    log_print("=" * 50)
    
    try:
        # 1. í•„ìˆ˜ ë„êµ¬ í™•ì¸
        if not check_prerequisites():
            return False
        
        # 2. ë³‘í•©ëœ ëª¨ë¸ í™•ì¸
        merged_model_dir = OUTPUT_DIR / "qwen3_finetune_merged"
        if not merged_model_dir.exists():
            log_print("âŒ ë³‘í•©ëœ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            log_print("   ë¨¼ì € 4ë‹¨ê³„ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”: python3 step4_merge_adapters.py")
            return False
        
        # 3. llama.cpp í™˜ê²½ êµ¬ì¶•
        llama_cpp_dir = setup_llama_cpp()
        if llama_cpp_dir is None:
            return False
        
        # 4. HuggingFace â†’ GGUF ë³€í™˜
        gguf_path = convert_hf_to_gguf(merged_model_dir, llama_cpp_dir)
        if gguf_path is None:
            return False
        
        # 5. GGUF ëª¨ë¸ ì–‘ìí™”
        quantized_path = quantize_gguf_model(gguf_path, llama_cpp_dir)
        if quantized_path is None:
            return False
        
        # 6. Ollama ì„¤ì¹˜ í™•ì¸
        if not check_ollama_installation():
            log_print("   Ollama ì„¤ì¹˜ í›„ ë‹¤ì‹œ ì‹¤í–‰í•˜ì„¸ìš”.")
            return False
        
        # 7. Ollama Modelfile ìƒì„±
        modelfile_path = create_ollama_modelfile(quantized_path)
        
        # 8. Ollamaì— ëª¨ë¸ ë“±ë¡
        model_name = register_ollama_model(modelfile_path)
        if model_name is None:
            return False
        
        # 9. ëª¨ë¸ í…ŒìŠ¤íŠ¸
        if not test_ollama_model(model_name):
            log_print("   âš ï¸ ëª¨ë¸ í…ŒìŠ¤íŠ¸ì— ë¬¸ì œê°€ ìˆì§€ë§Œ ë“±ë¡ì€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        # 10. ì„ì‹œ íŒŒì¼ ì •ë¦¬
        cleanup_temp_files()

        # GGUF íŒŒì¼ì„ models ë””ë ‰í† ë¦¬ë¡œ ì´ë™
        try:
            MODELS_DIR.mkdir(exist_ok=True)
            final_gguf_path = MODELS_DIR / quantized_path.name
            shutil.move(str(quantized_path), str(final_gguf_path))
            log_print(f"   âœ… GGUF íŒŒì¼ì„ models ë””ë ‰í† ë¦¬ë¡œ ì´ë™: {final_gguf_path}")
        except Exception as e:
            log_print(f"   âš ï¸ GGUF íŒŒì¼ ì´ë™ ì‹¤íŒ¨: {e}")

        log_print("=" * 50)
        log_print("âœ… 5ë‹¨ê³„ ì™„ë£Œ!")
        log_print("ğŸ“‹ ê²°ê³¼:")
        log_print(f"   GGUF ëª¨ë¸: {final_gguf_path if 'final_gguf_path' in locals() else quantized_path}")
        log_print(f"   Ollama ëª¨ë¸ëª…: {model_name}")
        log_print(f"   ì–‘ìí™”: {GGUF_CONFIG['quantization_type'].upper()}")
        log_print("\nğŸ“‹ ì‚¬ìš© ë°©ë²•:")
        log_print(f"   ollama run {model_name}")
        log_print(f"   ë˜ëŠ” API: http://localhost:11434/api/generate")
        log_print("\nğŸ“‹ ë‹¤ìŒ ë‹¨ê³„ ì•ˆë‚´:")
        log_print("   - 6ë‹¨ê³„ ì‹¤í–‰: python3 step6_optimize_deployment.py")
        log_print("   - API ì„œë²„ ìµœì í™” ë° ë°°í¬ ì¤€ë¹„")
        return True
        
    except Exception as e:
        log_print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1) 