#!/usr/bin/env python3
"""
ğŸ§ª 3ë‹¨ê³„: í•™ìŠµ ê²°ê³¼ ê²€ì¦ (30ë¶„)

ëª©ì :
- íŒŒì¸íŠœë‹ íš¨ê³¼ ì •ëŸ‰ì /ì •ì„±ì  ê²€ì¦
- í•™ìŠµ ë°ì´í„°ì™€ì˜ ì¼ì¹˜ë„ í™•ì¸
- ê³¼ì í•© ì—¬ë¶€ íŒë‹¨
"""

import os
import sys
import time
import json
import torch
import gc
from pathlib import Path
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from peft import PeftModel
import psutil
import GPUtil

# ì„¤ì • íŒŒì¼ import
from config import *

LOG_DIR = os.path.join(os.path.dirname(__file__), 'logs')
LOG_FILE = os.path.join(LOG_DIR, 'step3_validate_results.log')
os.makedirs(LOG_DIR, exist_ok=True)
def log_print(*args, **kwargs):
    print(*args, **kwargs)
    with open(LOG_FILE, 'a', encoding='utf-8') as f:
        print(*args, **kwargs, file=f)

def load_finetuned_model():
    """íŒŒì¸íŠœë‹ëœ ëª¨ë¸ ë¡œë“œ"""
    log_print("ğŸ¤– íŒŒì¸íŠœë‹ëœ ëª¨ë¸ ë¡œë“œ ì¤‘...")
    
    lora_adapter_path = OUTPUT_DIR / "qwen3_lora_adapters"
    
    if not lora_adapter_path.exists():
        log_print("âŒ LoRA ì–´ëŒ‘í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        log_print("   ë¨¼ì € 2ë‹¨ê³„ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”: python3 step2_train_qlora.py")
        return None, None
    
    try:
        # BitsAndBytesConfig ì„¤ì •
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.bfloat16,
            bnb_4bit_use_double_quant=False,
        )
    
        # ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ
        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
        model = AutoModelForCausalLM.from_pretrained(
            MODEL_NAME,
            quantization_config=bnb_config,
            torch_dtype=torch.bfloat16,
            device_map="auto",
            trust_remote_code=True
        )
    
        # LoRA ì–´ëŒ‘í„° ë¡œë“œ
        model = PeftModel.from_pretrained(model, str(lora_adapter_path))
        log_print(f"   âœ… LoRA ì–´ëŒ‘í„° ë¡œë“œ ì™„ë£Œ: {lora_adapter_path}")
    
        # ì¶”ë¡  ëª¨ë“œë¡œ ì„¤ì •
        model.eval()
        return model, tokenizer
    except Exception as e:
        log_print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return None, None

def load_test_data():
    """í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ"""
    log_print("ğŸ“Š í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ ì¤‘...")
    
    # ì ˆëŒ€ ê²½ë¡œë¡œ ë³€ê²½
    data_path = PROJECT_ROOT / "data" / "korean_tech_qa.json"
    with open(data_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    test_cases = []
    for item in data:
        messages = item['messages']
        system_prompt = messages[0]['content']
        user_question = messages[1]['content']
        expected_answer = messages[2]['content']
        
        test_cases.append({
            "system": system_prompt,
            "question": user_question,
            "expected": expected_answer
        })
    
    log_print(f"   ë¡œë“œëœ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤: {len(test_cases)}ê°œ")
    return test_cases

def generate_response(model, tokenizer, system_prompt, question, max_new_tokens=512):
    """ì‘ë‹µ ìƒì„±"""
    # Qwen3 chat template ì‚¬ìš©
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": question}
    ]
    
    prompt = tokenizer.apply_chat_template(
        messages, 
        tokenize=False, 
        add_generation_prompt=True
    )
    
    # í† í¬ë‚˜ì´ì§•
    inputs = tokenizer([prompt], return_tensors="pt").to(model.device)
    
    start_time = time.time()
    
    # ì‘ë‹µ ìƒì„±
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            temperature=0.7,
            do_sample=True,
            top_p=0.9,
            repetition_penalty=1.1,
            pad_token_id=tokenizer.eos_token_id,
            eos_token_id=tokenizer.eos_token_id
        )
    
    generation_time = time.time() - start_time
    
    # ì‘ë‹µ ë””ì½”ë”© (ì…ë ¥ í”„ë¡¬í”„íŠ¸ ì œì™¸)
    generated_tokens = outputs[0][inputs.input_ids.shape[1]:]
    response = tokenizer.decode(generated_tokens, skip_special_tokens=True)
    
    # í† í° ìˆ˜ ê³„ì‚°
    num_tokens = len(generated_tokens)
    tokens_per_second = num_tokens / generation_time if generation_time > 0 else 0
    
    return response.strip(), generation_time, tokens_per_second

def calculate_similarity(text1, text2):
    """í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê³„ì‚° (ê°„ë‹¨í•œ í‚¤ì›Œë“œ ë§¤ì¹­)"""
    # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ ìœ ì‚¬ë„ (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ ë°©ë²• ì‚¬ìš© ê°€ëŠ¥)
    words1 = set(text1.lower().split())
    words2 = set(text2.lower().split())
    
    intersection = words1.intersection(words2)
    union = words1.union(words2)
    
    jaccard_similarity = len(intersection) / len(union) if union else 0
    return jaccard_similarity

def test_training_data_accuracy(model, tokenizer, test_cases):
    """í•™ìŠµ ë°ì´í„° ì •í™•ë„ í…ŒìŠ¤íŠ¸"""
    log_print("ğŸ¯ í•™ìŠµ ë°ì´í„° ì •í™•ë„ í…ŒìŠ¤íŠ¸ ì¤‘...")
    log_print("-" * 50)
    
    total_cases = len(test_cases)
    similarities = []
    response_times = []
    tokens_per_sec_list = []
    
    for i, case in enumerate(test_cases):
        log_print(f"\nğŸ“ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ {i+1}/{total_cases}")
        log_print(f"ì§ˆë¬¸: {case['question'][:50]}...")
        
        # ì‘ë‹µ ìƒì„±
        response, gen_time, tokens_per_sec = generate_response(
            model, tokenizer, case['system'], case['question']
        )
        
        # ìœ ì‚¬ë„ ê³„ì‚°
        similarity = calculate_similarity(response, case['expected'])
        similarities.append(similarity)
        response_times.append(gen_time)
        tokens_per_sec_list.append(tokens_per_sec)
        
        log_print(f"ìƒì„± ì‹œê°„: {gen_time:.1f}ì´ˆ ({tokens_per_sec:.2f} í† í°/ì´ˆ)")
        log_print(f"ìœ ì‚¬ë„: {similarity:.3f}")
        log_print(f"ìƒì„± ì‘ë‹µ: {response[:100]}...")
        log_print(f"ê¸°ëŒ€ ì‘ë‹µ: {case['expected'][:100]}...")
    
    # ì „ì²´ í†µê³„
    avg_similarity = sum(similarities) / len(similarities)
    avg_response_time = sum(response_times) / len(response_times)
    avg_tokens_per_sec = sum(tokens_per_sec_list) / len(tokens_per_sec_list)
    
    log_print("\n" + "=" * 50)
    log_print("ğŸ“Š í•™ìŠµ ë°ì´í„° ì •í™•ë„ ê²°ê³¼:")
    log_print(f"   í‰ê·  ìœ ì‚¬ë„: {avg_similarity:.3f} ({avg_similarity*100:.1f}%)")
    log_print(f"   í‰ê·  ì‘ë‹µ ì‹œê°„: {avg_response_time:.1f}ì´ˆ")
    log_print(f"   í‰ê·  ìƒì„± ì†ë„: {avg_tokens_per_sec:.2f} í† í°/ì´ˆ")
    
    return {
        "avg_similarity": avg_similarity,
        "avg_response_time": avg_response_time,
        "avg_tokens_per_sec": avg_tokens_per_sec,
        "individual_similarities": similarities
    }

def test_generalization(model, tokenizer):
    """ì¼ë°˜í™” ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
    log_print("\nğŸ” ì¼ë°˜í™” ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì¤‘...")
    log_print("-" * 50)
    
    # í•™ìŠµí•˜ì§€ ì•Šì€ ì§ˆë¬¸ë“¤
    unseen_questions = [
        {
            "system": "ë‹¹ì‹ ì€ íŒŒì´ì¬ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ëª…í™•í•˜ê³  ì‹¤ìš©ì ì¸ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.",
            "question": "íŒŒì´ì¬ì—ì„œ ì œë„ˆë ˆì´í„°ì™€ ì¼ë°˜ í•¨ìˆ˜ì˜ ì°¨ì´ì ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”."
        },
        {
            "system": "ë‹¹ì‹ ì€ ë¨¸ì‹ ëŸ¬ë‹ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì‹¤ë¬´ì— ë„ì›€ë˜ëŠ” ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.",
            "question": "ë”¥ëŸ¬ë‹ì—ì„œ ë°°ì¹˜ ì •ê·œí™”(Batch Normalization)ì˜ ì—­í• ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”."
        },
        {
            "system": "ë‹¹ì‹ ì€ ì›¹ê°œë°œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì‹¤ìš©ì ì´ê³  ë³´ì•ˆì„ ê³ ë ¤í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.",
            "question": "ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ì—ì„œ CORS(Cross-Origin Resource Sharing) ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ë°©ë²•ì„ ì•Œë ¤ì£¼ì„¸ìš”."
        }
    ]
    
    generalization_results = []
    
    for i, case in enumerate(unseen_questions):
        log_print(f"\nğŸ” ì¼ë°˜í™” í…ŒìŠ¤íŠ¸ {i+1}/{len(unseen_questions)}")
        log_print(f"ì§ˆë¬¸: {case['question']}")
        
        response, gen_time, tokens_per_sec = generate_response(
            model, tokenizer, case['system'], case['question']
        )
        
        log_print(f"ìƒì„± ì‹œê°„: {gen_time:.1f}ì´ˆ ({tokens_per_sec:.2f} í† í°/ì´ˆ)")
        log_print(f"ì‘ë‹µ: {response[:200]}...")
        
        # í•œêµ­ì–´ í’ˆì§ˆ í‰ê°€ (ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±)
        korean_quality = evaluate_korean_quality(response)
        log_print(f"í•œêµ­ì–´ í’ˆì§ˆ: {korean_quality}/5")
        
        generalization_results.append({
            "question": case['question'],
            "response": response,
            "generation_time": gen_time,
            "tokens_per_sec": tokens_per_sec,
            "korean_quality": korean_quality
        })
    
    return generalization_results

def evaluate_korean_quality(text):
    """í•œêµ­ì–´ í’ˆì§ˆ í‰ê°€ (ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±)"""
    score = 5  # ê¸°ë³¸ ì ìˆ˜
    
    # í•œê¸€ ë¹„ìœ¨ í™•ì¸
    korean_chars = sum(1 for char in text if 'ê°€' <= char <= 'í£')
    total_chars = len([char for char in text if char.isalpha()])
    
    if total_chars > 0:
        korean_ratio = korean_chars / total_chars
        if korean_ratio < 0.3:
            score -= 2
        elif korean_ratio < 0.5:
            score -= 1
    
    # ê¸°ë³¸ì ì¸ êµ¬ì¡° í™•ì¸
    if "**" in text or "```" in text:  # ë§ˆí¬ë‹¤ìš´ í¬ë§· ì‚¬ìš©
        score += 1
    
    if len(text) < 50:  # ë„ˆë¬´ ì§§ì€ ë‹µë³€
        score -= 1
    
    return max(1, min(5, score))

def benchmark_inference_speed(model, tokenizer):
    """ì¶”ë¡  ì†ë„ ë²¤ì¹˜ë§ˆí¬"""
    log_print("\nâš¡ ì¶”ë¡  ì†ë„ ë²¤ì¹˜ë§ˆí¬ ì¤‘...")
    log_print("-" * 50)
    
    test_prompt = "ë‹¹ì‹ ì€ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤."
    test_question = "ì¸ê³µì§€ëŠ¥ì— ëŒ€í•´ ê°„ë‹¨íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”."
    
    # ì—¬ëŸ¬ ë²ˆ ì¸¡ì •í•˜ì—¬ í‰ê·  êµ¬í•˜ê¸°
    times = []
    token_counts = []
    
    for i in range(5):
        log_print(f"   í…ŒìŠ¤íŠ¸ {i+1}/5 ì§„í–‰ ì¤‘...")
        response, gen_time, tokens_per_sec = generate_response(
            model, tokenizer, test_prompt, test_question, max_new_tokens=100
        )
        times.append(gen_time)
        token_counts.append(len(tokenizer.encode(response)))
    
    avg_time = sum(times) / len(times)
    avg_tokens = sum(token_counts) / len(token_counts)
    avg_speed = avg_tokens / avg_time if avg_time > 0 else 0
    
    log_print(f"   í‰ê·  ìƒì„± ì‹œê°„: {avg_time:.2f}ì´ˆ")
    log_print(f"   í‰ê·  í† í° ìˆ˜: {avg_tokens:.1f}")
    log_print(f"   í‰ê·  ì†ë„: {avg_speed:.2f} í† í°/ì´ˆ")
    
    return avg_speed

def save_validation_results(accuracy_results, generalization_results, benchmark_speed):
    """ê²€ì¦ ê²°ê³¼ ì €ì¥"""
    results = {
        "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
        "accuracy": accuracy_results,
        "generalization": generalization_results,
        "benchmark_speed": benchmark_speed,
        "model_config": {
            "model_name": MODEL_NAME,
            "lora_config": QLORA_CONFIG
        }
    }
    
    results_file = OUTPUT_DIR / "validation_results.json"
    with open(results_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    log_print(f"\nğŸ’¾ ê²€ì¦ ê²°ê³¼ ì €ì¥: {results_file}")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    log_print("ğŸ§ª 3ë‹¨ê³„: í•™ìŠµ ê²°ê³¼ ê²€ì¦ ì‹œì‘")
    log_print("=" * 50)
    
    try:
        # 1. íŒŒì¸íŠœë‹ëœ ëª¨ë¸ ë¡œë“œ
        model, tokenizer = load_finetuned_model()
        if model is None:
            return False
        
        # 2. í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
        test_cases = load_test_data()
        
        # 3. í•™ìŠµ ë°ì´í„° ì •í™•ë„ í…ŒìŠ¤íŠ¸
        accuracy_results = test_training_data_accuracy(model, tokenizer, test_cases)
        
        # 4. ì¼ë°˜í™” ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
        generalization_results = test_generalization(model, tokenizer)
        
        # 5. ì¶”ë¡  ì†ë„ ë²¤ì¹˜ë§ˆí¬
        benchmark_speed = benchmark_inference_speed(model, tokenizer)
        
        # 6. ê²°ê³¼ ì €ì¥
        save_validation_results(accuracy_results, generalization_results, benchmark_speed)
        
        # 7. ìµœì¢… í‰ê°€
        log_print("\n" + "=" * 50)
        log_print("ğŸ“‹ ìµœì¢… ê²€ì¦ ê²°ê³¼:")
        similarity_score = accuracy_results['avg_similarity'] * 100
        
        if similarity_score >= 90:
            log_print("   âœ… í•™ìŠµ íš¨ê³¼: ìš°ìˆ˜ (90%+)")
        elif similarity_score >= 70:
            log_print("   âš ï¸ í•™ìŠµ íš¨ê³¼: ë³´í†µ (70-90%)")
        else:
            log_print("   âŒ í•™ìŠµ íš¨ê³¼: ë¶€ì¡± (<70%)")
        
        if benchmark_speed >= 10:
            log_print("   âœ… ì¶”ë¡  ì†ë„: ì‹¤ìš©ì  (10+ í† í°/ì´ˆ)")
        elif benchmark_speed >= 1:
            log_print("   âš ï¸ ì¶”ë¡  ì†ë„: ëŠë¦¼ (1-10 í† í°/ì´ˆ)")  
        else:
            log_print("   âŒ ì¶”ë¡  ì†ë„: ë§¤ìš° ëŠë¦¼ (<1 í† í°/ì´ˆ)")
        
        log_print("\nğŸ“‹ ë‹¤ìŒ ë‹¨ê³„ ì•ˆë‚´:")
        log_print("   - 4ë‹¨ê³„ ì‹¤í–‰: python3 step4_merge_adapters.py")
        
        return True
        
    except Exception as e:
        log_print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1) 